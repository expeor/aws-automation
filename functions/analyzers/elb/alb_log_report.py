"""ALB ë¡œê·¸ ë¶„ì„ - ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜

Excel/HTML ë¦¬í¬íŠ¸ ìƒì„± ë¡œì§
"""

from __future__ import annotations

import logging
import os
from datetime import datetime
from typing import Any

from rich.console import Console

console = Console()
logger = logging.getLogger(__name__)


def _generate_reports(ctx, analyzer, analysis_results: dict[str, Any], output_dir: str) -> dict[str, str]:
    """ì¶œë ¥ ì„¤ì •ì— ë”°ë¼ Excel/HTML ë³´ê³ ì„œ ìƒì„±

    Args:
        ctx: ExecutionContext (output_config í¬í•¨)
        analyzer: ALBLogAnalyzer ì¸ìŠ¤í„´ìŠ¤
        analysis_results: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬

    Returns:
        ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬ {"excel": "...", "html": "..."}
    """
    from functions.reports.log_analyzer import ALBExcelReporter

    report_paths: dict[str, str] = {}

    # ì¶œë ¥ ì„¤ì • í™•ì¸
    should_excel = ctx.should_output_excel() if hasattr(ctx, "should_output_excel") else True
    should_html = ctx.should_output_html() if hasattr(ctx, "should_output_html") else True

    # Excel ë³´ê³ ì„œ ìƒì„± (ê¸°ë³¸ - ìµœì í™”ëœ ìƒì„¸ ë¦¬í¬íŠ¸)
    if should_excel:
        report_filename = _generate_report_filename(analyzer, analysis_results)
        report_path = os.path.join(output_dir, report_filename)

        reporter = ALBExcelReporter(data=analysis_results, output_dir=output_dir)
        final_report_path = reporter.generate_report(report_path)
        report_paths["excel"] = final_report_path

    # HTML ë³´ê³ ì„œ ìƒì„± (ìš”ì•½ ëŒ€ì‹œë³´ë“œ)
    if should_html:
        console.print("   HTML ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        html_path = _generate_html_report(ctx, analyzer, analysis_results, output_dir)
        if html_path:
            console.print("   [green]âœ“ HTML ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ[/green]")
            report_paths["html"] = html_path

            # HTMLë§Œ ìƒì„±í•œ ê²½ìš° ë¸Œë¼ìš°ì €ì—ì„œ ìë™ ì—´ê¸°
            if not should_excel:
                from core.shared.io.html import open_in_browser

                output_config = ctx.get_output_config() if hasattr(ctx, "get_output_config") else None
                if output_config is None or output_config.auto_open:
                    open_in_browser(html_path)

    return report_paths


def _generate_html_report(ctx, analyzer, analysis_results: dict[str, Any], output_dir: str) -> str | None:
    """HTML ìš”ì•½ ë³´ê³ ì„œ ìƒì„±

    ALB ë¡œê·¸ ë¶„ì„ì˜ ì£¼ìš” ì§€í‘œë¥¼ ì‹œê°í™”í•˜ëŠ” HTML ëŒ€ì‹œë³´ë“œ ìƒì„±.
    Excel ë¦¬í¬íŠ¸ì˜ ìƒì„¸ ë°ì´í„°ë¥¼ ë³´ì™„í•˜ëŠ” ìš©ë„.

    ë ˆì´ì•„ì›ƒ:
        [ê°œìš” ì„¹ì…˜] ELB ìƒíƒœ ì½”ë“œ / ì •ìƒ ì‘ë‹µ ë¹„ìœ¨ ê²Œì´ì§€ / ì‹œê°„ëŒ€ë³„ ìš”ì²­ íŠ¸ë Œë“œ
        [íŠ¸ë˜í”½ ë¶„ì„ ì„¹ì…˜] êµ­ê°€ë³„ ë¶„í¬ / Top í´ë¼ì´ì–¸íŠ¸ IP / Top ìš”ì²­ URL
        [ìš”ì²­ ë¶„ì„ ì„¹ì…˜] HTTP ë©”ì„œë“œ / User-Agent / í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ ì½”ë“œ / Targetë³„ ë¶„í¬
        [ì„±ëŠ¥ ë¶„ì„ ì„¹ì…˜] ì‘ë‹µ ì‹œê°„ ë°±ë¶„ìœ„ / ì‘ë‹µ ì‹œê°„ êµ¬ê°„ ë¶„í¬ / ë°ì´í„° ì „ì†¡ëŸ‰ / Backend ìƒíƒœ ì½”ë“œ
        [ì—ëŸ¬ ë¶„ì„ ì„¹ì…˜] ELB vs Backend ì—ëŸ¬ / ì—ëŸ¬ ì›ì¸ / URLë³„ ì—ëŸ¬ìœ¨ / ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ìœ¨ / ì—ëŸ¬ íŠ¸ë Œë“œ
    """
    try:
        from core.shared.io.html import HTMLReport

        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        total_logs = analysis_results.get("log_lines_count", 0)
        alb_name = analysis_results.get("alb_name", "ALB")
        unique_ips = analysis_results.get("unique_client_ips", 0)

        # ì—ëŸ¬ ì¹´ìš´íŠ¸
        elb_2xx = analysis_results.get("elb_2xx_count", 0)
        elb_3xx = analysis_results.get("elb_3xx_count", 0)
        elb_4xx = analysis_results.get("elb_4xx_count", 0)
        elb_5xx = analysis_results.get("elb_5xx_count", 0)
        backend_4xx = analysis_results.get("backend_4xx_count", 0)
        backend_5xx = analysis_results.get("backend_5xx_count", 0)
        long_response = analysis_results.get("long_response_count", 0)

        # ì •ìƒ ì‘ë‹µ ë¹„ìœ¨ ê³„ì‚°
        total_responses = elb_2xx + elb_3xx + elb_4xx + elb_5xx
        success_rate = ((elb_2xx + elb_3xx) / total_responses * 100) if total_responses > 0 else 0

        # ì‹¤ì œ ìš”ì²­í•œ IP ì¤‘ ì•…ì„± IP ë§¤ì¹­
        client_ip_counts = analysis_results.get("client_ip_counts", {})
        abuse_ips_all = set(analysis_results.get("abuse_ips_list", []))
        matching_abuse_ips = [(ip, client_ip_counts.get(ip, 0)) for ip in client_ip_counts if ip in abuse_ips_all]
        matching_abuse_ips = sorted(matching_abuse_ips, key=lambda x: -x[1])
        abuse_count = len(matching_abuse_ips)

        # HTMLReport ìƒì„±
        subtitle = f"ë¶„ì„ ê¸°ê°„: {analyzer.start_datetime.strftime('%Y-%m-%d %H:%M')} ~ {analyzer.end_datetime.strftime('%Y-%m-%d %H:%M')}"
        report = HTMLReport(title=f"ALB ë¡œê·¸ ë¶„ì„: {alb_name}", subtitle=subtitle)

        # ìš”ì•½ ì¹´ë“œ
        report.add_summary(
            [
                ("ì´ ìš”ì²­", f"{total_logs:,}", None),
                ("ê³ ìœ  IP", f"{unique_ips:,}", None),
                ("ì •ìƒ ì‘ë‹µë¥ ", f"{success_rate:.1f}%", "success" if success_rate >= 99 else None),
                ("ELB 4xx", f"{elb_4xx:,}", "warning" if elb_4xx > 0 else None),
                ("ELB 5xx", f"{elb_5xx:,}", "danger" if elb_5xx > 0 else None),
                ("Backend 5xx", f"{backend_5xx:,}", "danger" if backend_5xx > 0 else None),
                ("ëŠë¦° ì‘ë‹µ (â‰¥1s)", f"{long_response:,}", "warning" if long_response > 0 else None),
                ("ì•…ì„± IP", f"{abuse_count:,}", "danger" if abuse_count > 0 else None),
            ]
        )

        # =============================================================================
        # [ê°œìš” ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸ“Š ê°œìš”")

        # 1. ELB ìƒíƒœ ì½”ë“œ ë¶„í¬ (ë„ë„› ì°¨íŠ¸)
        status_data = [
            ("2xx ì„±ê³µ", elb_2xx),
            ("3xx ë¦¬ë‹¤ì´ë ‰íŠ¸", elb_3xx),
            ("4xx í´ë¼ì´ì–¸íŠ¸ ì—ëŸ¬", elb_4xx),
            ("5xx ì„œë²„ ì—ëŸ¬", elb_5xx),
        ]
        status_data = [(name, count) for name, count in status_data if count > 0]
        if status_data:
            report.add_pie_chart("ELB ìƒíƒœ ì½”ë“œ ë¶„í¬", status_data, doughnut=True)

        # 2. ì •ìƒ ì‘ë‹µ ë¹„ìœ¨ ê²Œì´ì§€ (NEW)
        report.add_gauge_chart(
            "ì •ìƒ ì‘ë‹µ ë¹„ìœ¨",
            value=success_rate,
            max_value=100,
            thresholds=[(0.9, "#ee6666"), (0.95, "#fac858"), (1, "#91cc75")],
            unit="%",
        )

        # 3. ì‹œê°„ëŒ€ë³„ ìš”ì²­ íŠ¸ë Œë“œ (ë¼ì¸ ì°¨íŠ¸) - CloudWatch ìŠ¤íƒ€ì¼ ì ì‘í˜• í•´ìƒë„
        all_timestamps: list[datetime] = []
        is_error_list: list[int | float] = []

        for key in ["ELB 2xx Count", "ELB 3xx Count", "ELB 4xx Count", "ELB 5xx Count"]:
            log_data = analysis_results.get(key, {})
            if isinstance(log_data, dict):
                timestamps = log_data.get("timestamps", [])
                is_error = 1 if ("4xx" in key or "5xx" in key) else 0
                for ts in timestamps:
                    if ts and hasattr(ts, "timestamp"):
                        all_timestamps.append(ts)
                        is_error_list.append(is_error)

        if all_timestamps:
            values: dict[str, list[int | float]] = {
                "ì „ì²´ ìš”ì²­": [1] * len(all_timestamps),
                "ì—ëŸ¬ (4xx+5xx)": is_error_list,
            }
            report.add_time_series_chart(
                "ì‹œê°„ëŒ€ë³„ ìš”ì²­ íŠ¸ë Œë“œ",
                timestamps=all_timestamps,
                values=values,
                aggregation="sum",
                area=True,
            )

        # =============================================================================
        # [íŠ¸ë˜í”½ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸŒ íŠ¸ë˜í”½ ë¶„ì„")

        # 4. êµ­ê°€ë³„ ìš”ì²­ ë¶„í¬ (ë°” ì°¨íŠ¸)
        country_stats = analysis_results.get("country_statistics", {})
        if country_stats:
            sorted_countries = [
                (country, data.get("count", 0) if isinstance(data, dict) else data)
                for country, data in country_stats.items()
            ]
            sorted_countries = sorted(sorted_countries, key=lambda x: -x[1])[:15]
            if sorted_countries:
                countries = [c[0] for c in sorted_countries]
                counts = [c[1] for c in sorted_countries]
                report.add_bar_chart(
                    "êµ­ê°€ë³„ ìš”ì²­ ë¶„í¬",
                    categories=countries,
                    series=[("ìš”ì²­ ìˆ˜", counts)],
                    horizontal=True,
                )

        # 5. Top í´ë¼ì´ì–¸íŠ¸ IP (ë°” ì°¨íŠ¸)
        if client_ip_counts:
            sorted_ips = sorted(client_ip_counts.items(), key=lambda x: -x[1])[:10]
            if sorted_ips:
                ips = [ip for ip, _ in sorted_ips]
                counts = [count for _, count in sorted_ips]
                report.add_bar_chart(
                    "Top í´ë¼ì´ì–¸íŠ¸ IP",
                    categories=ips,
                    series=[("ìš”ì²­ ìˆ˜", counts)],
                    horizontal=True,
                )

        # 6. Top ìš”ì²­ URL (ë°” ì°¨íŠ¸)
        url_counts = analysis_results.get("request_url_counts", {})
        if url_counts:
            sorted_urls = sorted(url_counts.items(), key=lambda x: -x[1])[:15]
            if sorted_urls:
                urls = [url[:60] + "..." if len(url) > 60 else url for url, _ in sorted_urls]
                url_counts_list = [count for _, count in sorted_urls]
                report.add_bar_chart(
                    "Top ìš”ì²­ URL",
                    categories=urls,
                    series=[("ìš”ì²­ ìˆ˜", url_counts_list)],
                    horizontal=True,
                )

        # =============================================================================
        # [ìš”ì²­ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸ“ ìš”ì²­ ë¶„ì„")

        # 7. HTTP ë©”ì„œë“œ ë¶„í¬ (ë„ë„› ì°¨íŠ¸)
        request_url_details = analysis_results.get("request_url_details", {})
        if request_url_details:
            method_totals: dict[str, int] = {}
            for url_detail in request_url_details.values():
                if isinstance(url_detail, dict):
                    methods = url_detail.get("methods", {})
                    for method, cnt in methods.items():
                        if method and isinstance(cnt, int):
                            method_totals[method] = method_totals.get(method, 0) + cnt
            if method_totals:
                sorted_methods = sorted(method_totals.items(), key=lambda x: -x[1])
                method_data: list[tuple[str, int | float]] = [(m, c) for m, c in sorted_methods if m.strip()]
                if method_data:
                    report.add_pie_chart("HTTP ë©”ì„œë“œ ë¶„í¬", method_data, doughnut=True)

        # 8. Top User-Agent (ë°” ì°¨íŠ¸)
        user_agent_counts = analysis_results.get("user_agent_counts", {})
        if user_agent_counts:
            sorted_uas = sorted(user_agent_counts.items(), key=lambda x: -x[1])[:10]
            if sorted_uas:
                uas = [ua[:50] + "..." if len(ua) > 50 else ua for ua, _ in sorted_uas]
                counts = [count for _, count in sorted_uas]
                report.add_bar_chart(
                    "Top User-Agent",
                    categories=uas,
                    series=[("ìš”ì²­ ìˆ˜", counts)],
                    horizontal=True,
                )

        # 9. í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ ì½”ë“œ ë¶„í¬ (ë°” ì°¨íŠ¸)
        client_status = analysis_results.get("client_status_statistics", {})
        if client_status:
            status_totals: dict[str, int] = {}
            for ip_stats in client_status.values():
                if isinstance(ip_stats, dict):
                    for code, count in ip_stats.items():
                        if isinstance(count, int):
                            status_totals[code] = status_totals.get(code, 0) + count
            if status_totals:
                top_codes = sorted(status_totals.items(), key=lambda x: -x[1])[:10]
                if top_codes:
                    codes = [str(code) for code, _ in top_codes]
                    status_counts: list[int | float] = [count for _, count in top_codes]
                    status_series: list[tuple[str, list[int | float]]] = [("ìš”ì²­ ìˆ˜", status_counts)]
                    report.add_bar_chart(
                        "í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ ì½”ë“œ ë¶„í¬",
                        categories=codes,
                        series=status_series,
                    )

        # 10. Targetë³„ ìš”ì²­ ë¶„í¬ (ë°” ì°¨íŠ¸)
        target_request_stats = analysis_results.get("target_request_stats", {})
        if target_request_stats:
            sorted_targets = sorted(
                target_request_stats.items(),
                key=lambda x: x[1].get("total_requests", 0),
                reverse=True,
            )[:10]
            if sorted_targets:
                target_names = [name[:40] + "..." if len(name) > 40 else name for name, _ in sorted_targets]
                request_counts = [stats.get("total_requests", 0) for _, stats in sorted_targets]
                error_counts = [stats.get("error_count", 0) for _, stats in sorted_targets]
                report.add_bar_chart(
                    "Targetë³„ ìš”ì²­ ë¶„í¬",
                    categories=target_names,
                    series=[
                        ("ì •ìƒ ìš”ì²­", [r - e for r, e in zip(request_counts, error_counts, strict=True)]),
                        ("ì—ëŸ¬", error_counts),
                    ],
                    stacked=True,
                    horizontal=True,
                )

        # =============================================================================
        # [ì„±ëŠ¥ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("âš¡ ì„±ëŠ¥ ë¶„ì„")

        # NEW: TPS ì‹œê³„ì—´ ì°¨íŠ¸ (ì²˜ë¦¬ëŸ‰)
        tps_time_series = analysis_results.get("tps_time_series", [])
        if tps_time_series:
            tps_timestamps = [item.get("timestamp") for item in tps_time_series if item.get("timestamp")]
            tps_values = [item.get("tps", 0) for item in tps_time_series]
            if tps_timestamps and any(v > 0 for v in tps_values):
                bucket_minutes = analysis_results.get("bucket_minutes", 15)
                report.add_time_series_chart(
                    f"TPS ì¶”ì´ ({bucket_minutes}ë¶„ ë‹¨ìœ„)",
                    timestamps=tps_timestamps,
                    values={"TPS": tps_values},
                    aggregation="last",
                    area=True,
                )

        # NEW: ì‹œê°„ëŒ€ë³„ Latency ì¶”ì´ ì°¨íŠ¸ (P50/P90/P99)
        if tps_time_series:
            tps_timestamps = [item.get("timestamp") for item in tps_time_series if item.get("timestamp")]
            p50_values = [item.get("p50_ms", 0) for item in tps_time_series]
            p90_values = [item.get("p90_ms", 0) for item in tps_time_series]
            p99_values = [item.get("p99_ms", 0) for item in tps_time_series]
            if tps_timestamps and any(v > 0 for v in p50_values + p90_values + p99_values):
                bucket_minutes = analysis_results.get("bucket_minutes", 15)
                report.add_time_series_chart(
                    f"ì‘ë‹µ ì‹œê°„ ì¶”ì´ ({bucket_minutes}ë¶„ ë‹¨ìœ„)",
                    timestamps=tps_timestamps,
                    values={
                        "P50 (ms)": p50_values,
                        "P90 (ms)": p90_values,
                        "P99 (ms)": p99_values,
                    },
                    aggregation="last",
                    area=False,
                )

        # NEW: SLA ì¤€ìˆ˜ìœ¨ ê²Œì´ì§€ ì°¨íŠ¸ (3ê°œ)
        sla_compliance = analysis_results.get("sla_compliance", {})
        if sla_compliance:
            # 100ms ê¸°ì¤€
            if "under_100ms" in sla_compliance:
                sla_100ms = sla_compliance["under_100ms"]
                report.add_gauge_chart(
                    "SLA: 100ms ë¯¸ë§Œ ì‘ë‹µë¥ ",
                    value=sla_100ms.get("rate", 0),
                    max_value=100,
                    thresholds=[(0.9, "#ee6666"), (0.95, "#fac858"), (1, "#91cc75")],
                    unit="%",
                )

            # 500ms ê¸°ì¤€
            if "under_500ms" in sla_compliance:
                sla_500ms = sla_compliance["under_500ms"]
                report.add_gauge_chart(
                    "SLA: 500ms ë¯¸ë§Œ ì‘ë‹µë¥ ",
                    value=sla_500ms.get("rate", 0),
                    max_value=100,
                    thresholds=[(0.95, "#ee6666"), (0.99, "#fac858"), (1, "#91cc75")],
                    unit="%",
                )

            # 1s ê¸°ì¤€
            if "under_1s" in sla_compliance:
                sla_1s = sla_compliance["under_1s"]
                report.add_gauge_chart(
                    "SLA: 1ì´ˆ ë¯¸ë§Œ ì‘ë‹µë¥ ",
                    value=sla_1s.get("rate", 0),
                    max_value=100,
                    thresholds=[(0.99, "#ee6666"), (0.999, "#fac858"), (1, "#91cc75")],
                    unit="%",
                )

        # 11. ì‘ë‹µ ì‹œê°„ ë°±ë¶„ìœ„ìˆ˜ (ë°” ì°¨íŠ¸)
        response_time_percentiles = analysis_results.get("response_time_percentiles", {})
        if response_time_percentiles:
            percentile_labels = ["P50", "P90", "P95", "P99", "í‰ê· "]
            percentile_values = [
                round(response_time_percentiles.get("p50", 0) * 1000, 1),
                round(response_time_percentiles.get("p90", 0) * 1000, 1),
                round(response_time_percentiles.get("p95", 0) * 1000, 1),
                round(response_time_percentiles.get("p99", 0) * 1000, 1),
                round(response_time_percentiles.get("avg", 0) * 1000, 1),
            ]
            if any(v > 0 for v in percentile_values):
                report.add_bar_chart(
                    "ì‘ë‹µ ì‹œê°„ ë¶„í¬ (ms)",
                    categories=percentile_labels,
                    series=[("ì‘ë‹µ ì‹œê°„", percentile_values)],
                )

        # 12. ì‘ë‹µ ì‹œê°„ êµ¬ê°„ ë¶„í¬ - ë°” ì°¨íŠ¸ (DuckDBì—ì„œ ê³„ì‚°ëœ ë°ì´í„° ìš°ì„  ì‚¬ìš©)
        response_time_distribution = analysis_results.get("response_time_distribution", {})
        long_response_times = analysis_results.get("long_response_times", [])
        if response_time_distribution:
            # DuckDBì—ì„œ ê³„ì‚°ëœ ë¶„í¬ ì‚¬ìš©
            if any(v > 0 for v in response_time_distribution.values()):
                report.add_bar_chart(
                    "ì‘ë‹µ ì‹œê°„ êµ¬ê°„ ë¶„í¬",
                    categories=list(response_time_distribution.keys()),
                    series=[("ìš”ì²­ ìˆ˜", list(response_time_distribution.values()))],
                )
        else:
            # Fallback: long_response_timesì—ì„œ ê³„ì‚°
            if long_response_times or response_time_percentiles:
                # êµ¬ê°„ë³„ ì¹´ìš´íŠ¸ ê³„ì‚°
                buckets = {"<100ms": 0, "100-500ms": 0, "500ms-1s": 0, "1-3s": 0, ">3s": 0}

                # long_response_timesì—ì„œ ì‘ë‹µ ì‹œê°„ ì¶”ì¶œ
                for r in long_response_times:
                    rt = r.get("response_time")
                    if rt is not None:
                        rt_ms = rt * 1000
                        if rt_ms < 100:
                            buckets["<100ms"] += 1
                        elif rt_ms < 500:
                            buckets["100-500ms"] += 1
                        elif rt_ms < 1000:
                            buckets["500ms-1s"] += 1
                        elif rt_ms < 3000:
                            buckets["1-3s"] += 1
                        else:
                            buckets[">3s"] += 1

                # ê°’ì´ ìˆìœ¼ë©´ ì°¨íŠ¸ ì¶”ê°€
                if any(v > 0 for v in buckets.values()):
                    report.add_bar_chart(
                        "ì‘ë‹µ ì‹œê°„ êµ¬ê°„ ë¶„í¬",
                        categories=list(buckets.keys()),
                        series=[("ìš”ì²­ ìˆ˜", list(buckets.values()))],
                    )

        # 13. ë°ì´í„° ì „ì†¡ëŸ‰ (ë°” ì°¨íŠ¸)
        total_received = analysis_results.get("total_received_bytes") or 0
        total_sent = analysis_results.get("total_sent_bytes") or 0
        if total_received > 0 or total_sent > 0:

            def to_gb(b: int) -> float:
                return b / (1024**3) if b else 0

            transfer_data = [
                ("ìˆ˜ì‹  (Received)", to_gb(total_received)),
                ("ì†¡ì‹  (Sent)", to_gb(total_sent)),
            ]
            transfer_data = [(name, val) for name, val in transfer_data if val > 0]
            if transfer_data:
                report.add_bar_chart(
                    "ë°ì´í„° ì „ì†¡ëŸ‰ (GB)",
                    categories=[name for name, _ in transfer_data],
                    series=[("GB", [round(val, 2) for _, val in transfer_data])],
                )

        # 14. Backend ìƒíƒœ ì½”ë“œ ë¶„í¬ (ë„ë„› ì°¨íŠ¸)
        backend_status_data = [
            ("4xx í´ë¼ì´ì–¸íŠ¸ ì—ëŸ¬", backend_4xx),
            ("5xx ì„œë²„ ì—ëŸ¬", backend_5xx),
        ]
        backend_status_data = [(name, count) for name, count in backend_status_data if count > 0]
        if backend_status_data:
            report.add_pie_chart("Backend ìƒíƒœ ì½”ë“œ ë¶„í¬", backend_status_data, doughnut=True)

        # NEW: Targetë³„ Latency ë¹„êµ ì°¨íŠ¸
        target_latency_stats = analysis_results.get("target_latency_stats", {})
        if target_latency_stats:
            # Top 10 targets by request count
            sorted_targets = sorted(
                target_latency_stats.items(),
                key=lambda x: x[1].get("total_requests", 0),
                reverse=True,
            )[:10]
            if sorted_targets:
                target_names = [name[:40] + "..." if len(name) > 40 else name for name, _ in sorted_targets]
                p50_values = [stats.get("p50_ms", 0) for _, stats in sorted_targets]
                p90_values = [stats.get("p90_ms", 0) for _, stats in sorted_targets]
                p99_values = [stats.get("p99_ms", 0) for _, stats in sorted_targets]
                report.add_bar_chart(
                    "Targetë³„ ì‘ë‹µ ì‹œê°„ ë¹„êµ (ms)",
                    categories=target_names,
                    series=[
                        ("P50", p50_values),
                        ("P90", p90_values),
                        ("P99", p99_values),
                    ],
                    horizontal=True,
                )

        # NEW: ì²˜ë¦¬ ì‹œê°„ ë¶„í•´ ë¶„ì„ (Request/Target/Response)
        processing_time_breakdown = analysis_results.get("processing_time_breakdown", {})
        if processing_time_breakdown:
            req_data = processing_time_breakdown.get("request", {})
            target_data = processing_time_breakdown.get("target", {})
            resp_data = processing_time_breakdown.get("response", {})

            if any([req_data, target_data, resp_data]):
                # P50/P90/P99ë³„ ê° ë‹¨ê³„ì˜ ì²˜ë¦¬ ì‹œê°„
                report.add_bar_chart(
                    "ì²˜ë¦¬ ë‹¨ê³„ë³„ ì‘ë‹µ ì‹œê°„ (ms)",
                    categories=["P50", "P90", "P99"],
                    series=[
                        (
                            "Request (ALB)",
                            [req_data.get("p50_ms", 0), req_data.get("p90_ms", 0), req_data.get("p99_ms", 0)],
                        ),
                        (
                            "Target (Backend)",
                            [target_data.get("p50_ms", 0), target_data.get("p90_ms", 0), target_data.get("p99_ms", 0)],
                        ),
                        (
                            "Response (ì „ì†¡)",
                            [resp_data.get("p50_ms", 0), resp_data.get("p90_ms", 0), resp_data.get("p99_ms", 0)],
                        ),
                    ],
                )

        # NEW: ì—°ê²° ì‹¤íŒ¨ ë¶„ì„
        connection_failures = analysis_results.get("connection_failures", {})
        if connection_failures and connection_failures.get("target_failures", 0) > 0:
            failure_data = [
                ("Request ì‹¤íŒ¨", connection_failures.get("request_failures", 0)),
                ("Target ì—°ê²° ì‹¤íŒ¨", connection_failures.get("target_failures", 0)),
                ("Response ì „ì†¡ ì‹¤íŒ¨", connection_failures.get("response_failures", 0)),
            ]
            failure_data = [(name, count) for name, count in failure_data if count > 0]
            if failure_data:
                report.add_pie_chart("ì—°ê²° ì‹¤íŒ¨ ìœ í˜• ë¶„í¬", failure_data, doughnut=True)

            # Targetë³„ ì‹¤íŒ¨ ìƒì„¸
            target_failures_detail = connection_failures.get("target_failures_detail", [])
            if target_failures_detail:
                target_names = [
                    f"{d['target_group']}({d['target']})" if d["target_group"] else d["target"]
                    for d in target_failures_detail[:10]
                ]
                failure_counts = [d["count"] for d in target_failures_detail[:10]]
                report.add_bar_chart(
                    "Targetë³„ ì—°ê²° ì‹¤íŒ¨",
                    categories=target_names,
                    series=[("ì‹¤íŒ¨ ìˆ˜", failure_counts)],
                    horizontal=True,
                )

        # ëŠë¦° ì‘ë‹µ í…Œì´ë¸” (ì„±ëŠ¥ ë¶„ì„ ì„¹ì…˜)
        if long_response_times:
            slow_rows = []
            for r in long_response_times[:100]:
                response_time = r.get("response_time")
                response_time_str = f"{response_time:.3f}" if response_time is not None else "-"
                slow_rows.append(
                    [
                        str(r.get("timestamp") or "")[:19],
                        r.get("client_ip") or "",
                        (r.get("request") or "")[:50],
                        response_time_str,
                        r.get("elb_status_code") or "",
                        r.get("target_status_code") or "",
                    ]
                )
            report.add_table(
                title="ğŸ¢ ëŠë¦° ì‘ë‹µ Top 100",
                headers=["ì‹œê°„", "Client IP", "URL", "ì‘ë‹µ ì‹œê°„(s)", "ELB Status", "Target Status"],
                rows=slow_rows,
                page_size=20,
            )

        # =============================================================================
        # [í”„ë¡œí† ì½œ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸ” í”„ë¡œí† ì½œ ë¶„ì„")

        # NEW: HTTP ë²„ì „ ë¶„í¬
        http_version_distribution = analysis_results.get("http_version_distribution", {})
        if http_version_distribution:
            http_version_data = [(version, count) for version, count in http_version_distribution.items() if count > 0]
            if http_version_data:
                report.add_pie_chart("HTTP í”„ë¡œí† ì½œ ë²„ì „ ë¶„í¬", http_version_data, doughnut=True)

        # NEW: SSL/TLS í”„ë¡œí† ì½œ ë¶„í¬
        ssl_stats = analysis_results.get("ssl_stats", {})
        if ssl_stats:
            protocol_dist = ssl_stats.get("protocol_distribution", {})
            if protocol_dist:
                tls_data = [(proto, count) for proto, count in protocol_dist.items() if count > 0]
                if tls_data:
                    report.add_pie_chart("TLS í”„ë¡œí† ì½œ ë²„ì „ ë¶„í¬", tls_data, doughnut=True)

            cipher_dist = ssl_stats.get("cipher_distribution", {})
            if cipher_dist:
                cipher_names = list(cipher_dist.keys())[:10]
                cipher_counts = [cipher_dist[c] for c in cipher_names]
                if cipher_names:
                    report.add_bar_chart(
                        "ì•”í˜¸ ìŠ¤ìœ„íŠ¸ Top 10",
                        categories=cipher_names,
                        series=[("ìš”ì²­ ìˆ˜", cipher_counts)],
                        horizontal=True,
                    )

            # ì·¨ì•½ TLS ê²½ê³ 
            weak_tls_clients = ssl_stats.get("weak_tls_clients", [])
            if weak_tls_clients:
                report.add_table(
                    title=f"âš ï¸ ì·¨ì•½ TLS ì‚¬ìš© í´ë¼ì´ì–¸íŠ¸ ({len(weak_tls_clients)}ê°œ)",
                    headers=["í´ë¼ì´ì–¸íŠ¸ IP", "TLS ë²„ì „", "ìš”ì²­ ìˆ˜"],
                    rows=[[c["client_ip"], c["protocol"], c["count"]] for c in weak_tls_clients[:20]],
                    page_size=10,
                )

        # NEW: Actions ë¶„í¬
        actions_stats = analysis_results.get("actions_stats", {})
        if actions_stats:
            # 'None'ê³¼ 'Forward'ë¥¼ ì œì™¸í•œ ì˜ë¯¸ ìˆëŠ” ì•¡ì…˜ë§Œ í‘œì‹œ
            meaningful_actions = [
                (action, count)
                for action, count in actions_stats.items()
                if action not in ("None", "Forward", "Other") and count > 0
            ]
            if meaningful_actions:
                report.add_pie_chart("ALB Actions ë¶„í¬ (WAF/ì¸ì¦/ë¦¬ë‹¤ì´ë ‰íŠ¸)", meaningful_actions, doughnut=True)

        # =============================================================================
        # [ë³´ì•ˆ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸ›¡ï¸ ë³´ì•ˆ ë¶„ì„")

        # NEW: Classification ë¶„í¬ (Desync íƒì§€)
        classification_stats = analysis_results.get("classification_stats", {})
        if classification_stats:
            class_dist = classification_stats.get("distribution", {})
            if class_dist:
                # Ambiguous/Severeê°€ ìˆìœ¼ë©´ ê²½ê³ 
                ambiguous_count = class_dist.get("Ambiguous", 0)
                severe_count = class_dist.get("Severe", 0)

                if ambiguous_count > 0 or severe_count > 0:
                    class_data = [(cls, count) for cls, count in class_dist.items() if count > 0]
                    report.add_pie_chart("HTTP ìš”ì²­ ë¶„ë¥˜ (Desync íƒì§€)", class_data, doughnut=True)

            # ë³´ì•ˆ ì´ë²¤íŠ¸ í…Œì´ë¸” (ì „ì²´ í‘œì‹œ)
            security_events = classification_stats.get("security_events", [])
            if security_events:
                report.add_table(
                    title=f"ğŸš¨ ë³´ì•ˆ ì´ë²¤íŠ¸ (Ambiguous/Severe ìš”ì²­) - {len(security_events)}ê±´",
                    headers=["ì‹œê°„", "í´ë¼ì´ì–¸íŠ¸ IP", "ë¶„ë¥˜", "ì‚¬ìœ ", "URL"],
                    rows=[
                        [
                            e["timestamp"].strftime("%Y-%m-%d %H:%M:%S")
                            if hasattr(e["timestamp"], "strftime")
                            else str(e["timestamp"]),
                            e["client_ip"],
                            e["classification"],
                            e["reason"][:30] + "..." if len(e.get("reason", "")) > 30 else e.get("reason", ""),
                            e["url"][:50] + "..." if len(e.get("url", "")) > 50 else e.get("url", ""),
                        ]
                        for e in security_events
                    ],
                    page_size=50,
                )

        # ì•…ì„± IP í…Œì´ë¸” (ë³´ì•ˆ ë¶„ì„ ì„¹ì…˜)
        if matching_abuse_ips:
            report.add_table(
                title=f"ğŸš« ì•…ì„± IP ëª©ë¡ ({len(matching_abuse_ips)}ê°œ)",
                headers=["IP ì£¼ì†Œ", "ìš”ì²­ ìˆ˜", "ìƒíƒœ"],
                rows=[[ip, count, "AbuseIPDB ë“±ë¡"] for ip, count in matching_abuse_ips[:50]],
                page_size=20,
            )

        # =============================================================================
        # [ì—ëŸ¬ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸš¨ ì—ëŸ¬ ë¶„ì„")

        # 15. ELB vs Backend ì—ëŸ¬ ë¹„êµ (ë°” ì°¨íŠ¸)
        if elb_4xx > 0 or elb_5xx > 0 or backend_4xx > 0 or backend_5xx > 0:
            report.add_bar_chart(
                "ELB vs Backend ì—ëŸ¬ ë¹„êµ",
                categories=["4xx ì—ëŸ¬", "5xx ì—ëŸ¬"],
                series=[
                    ("ELB", [elb_4xx, elb_5xx]),
                    ("Backend", [backend_4xx, backend_5xx]),
                ],
            )

        # 16. ì—ëŸ¬ ì›ì¸ ë¶„í¬ (ë°” ì°¨íŠ¸)
        error_reason_counts = analysis_results.get("error_reason_counts", {})
        if error_reason_counts:
            sorted_reasons = sorted(error_reason_counts.items(), key=lambda x: -x[1])[:10]
            if sorted_reasons:
                reasons = [reason for reason, _ in sorted_reasons]
                counts = [count for _, count in sorted_reasons]
                report.add_bar_chart(
                    "ì—ëŸ¬ ì›ì¸ ë¶„í¬",
                    categories=reasons,
                    series=[("ê±´ìˆ˜", counts)],
                    horizontal=True,
                )

        # 17. URLë³„ ì—ëŸ¬ìœ¨ (ë°” ì°¨íŠ¸)
        url_error_stats = analysis_results.get("url_error_stats", {})
        if url_error_stats:
            sorted_urls = sorted(
                [(url, stats) for url, stats in url_error_stats.items() if stats.get("error_count", 0) > 0],
                key=lambda x: x[1].get("error_rate", 0),
                reverse=True,
            )[:15]
            if sorted_urls:
                url_names = [url[:50] + "..." if len(url) > 50 else url for url, _ in sorted_urls]
                error_rates = [stats.get("error_rate", 0) for _, stats in sorted_urls]
                report.add_bar_chart(
                    "URLë³„ ì—ëŸ¬ìœ¨ (%)",
                    categories=url_names,
                    series=[("ì—ëŸ¬ìœ¨", error_rates)],
                    horizontal=True,
                )

        # 18. ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ìœ¨ ì¶”ì´ (NEW) - ë¼ì¸ ì°¨íŠ¸
        error_timestamps: list[datetime] = []
        error_types: list[str] = []

        for key, error_type in [("ELB 4xx Count", "4xx"), ("ELB 5xx Count", "5xx")]:
            log_data = analysis_results.get(key, {})
            if isinstance(log_data, dict):
                timestamps = log_data.get("timestamps", [])
                for ts in timestamps:
                    if ts and hasattr(ts, "timestamp"):
                        error_timestamps.append(ts)
                        error_types.append(error_type)

        # ì—ëŸ¬ìœ¨ ì¶”ì´ ê³„ì‚° (ì‹œê°„ ë²„í‚·ë³„)
        if all_timestamps and error_timestamps:
            from collections import defaultdict

            # ë²„í‚· í¬ê¸° ê²°ì • (ì‹œê°„ ë²”ìœ„ì— ë”°ë¼)
            min_time = min(all_timestamps) if all_timestamps else datetime.now()
            max_time = max(all_timestamps) if all_timestamps else datetime.now()
            total_seconds = (max_time - min_time).total_seconds()
            total_hours = total_seconds / 3600

            if total_hours <= 3:
                bucket_minutes = 5
            elif total_hours <= 24:
                bucket_minutes = 15
            elif total_hours <= 24 * 7:
                bucket_minutes = 60
            else:
                bucket_minutes = 240

            bucket_seconds = bucket_minutes * 60

            # ë²„í‚·ë³„ ì „ì²´ ìš”ì²­ê³¼ ì—ëŸ¬ ì¹´ìš´íŠ¸
            total_buckets: dict[datetime, int] = defaultdict(int)
            error_buckets: dict[datetime, int] = defaultdict(int)

            for ts in all_timestamps:
                bucket_start = datetime.fromtimestamp((ts.timestamp() // bucket_seconds) * bucket_seconds)
                total_buckets[bucket_start] += 1

            for ts in error_timestamps:
                bucket_start = datetime.fromtimestamp((ts.timestamp() // bucket_seconds) * bucket_seconds)
                error_buckets[bucket_start] += 1

            # ì—ëŸ¬ìœ¨ ê³„ì‚°
            if total_buckets:
                sorted_buckets = sorted(total_buckets.keys())
                error_rates_time: list[float] = []

                for bucket in sorted_buckets:
                    total = total_buckets[bucket]
                    errors = error_buckets.get(bucket, 0)
                    rate = (errors / total * 100) if total > 0 else 0
                    error_rates_time.append(round(rate, 2))

                # ì‹œê°„ í¬ë§·
                if total_hours <= 24:
                    time_format = "%H:%M"
                elif total_hours <= 24 * 7:
                    time_format = "%m/%d %H:%M"
                else:
                    time_format = "%m/%d"

                categories = [ts.strftime(time_format) for ts in sorted_buckets]

                if any(r > 0 for r in error_rates_time):
                    report.add_line_chart(
                        f"ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ìœ¨ ì¶”ì´ ({bucket_minutes}ë¶„ ë‹¨ìœ„)",
                        categories=categories,
                        series=[("ì—ëŸ¬ìœ¨ (%)", error_rates_time)],
                        area=True,
                        smooth=True,
                    )

        # 19. ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ íŠ¸ë Œë“œ (ë¼ì¸ ì°¨íŠ¸)
        if error_timestamps:
            is_4xx: list[int | float] = [1 if t == "4xx" else 0 for t in error_types]
            is_5xx: list[int | float] = [1 if t == "5xx" else 0 for t in error_types]

            error_values: dict[str, list[int | float]] = {
                "4xx ì—ëŸ¬": is_4xx,
                "5xx ì—ëŸ¬": is_5xx,
            }
            report.add_time_series_chart(
                "ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ íŠ¸ë Œë“œ",
                timestamps=error_timestamps,
                values=error_values,
                aggregation="sum",
                area=True,
            )

        # =============================================================================
        # í…Œì´ë¸” ì„¹ì…˜
        # =============================================================================
        def format_bytes(b: int | None) -> str:
            if b is None:
                return "0 B"
            if b >= 1024**3:
                return f"{b / 1024**3:.2f} GB"
            elif b >= 1024**2:
                return f"{b / 1024**2:.2f} MB"
            elif b >= 1024:
                return f"{b / 1024:.2f} KB"
            return f"{b} B"

        # ë¶„ì„ ì •ë³´ í…Œì´ë¸”
        report.add_table(
            title="ë¶„ì„ ì •ë³´",
            headers=["í•­ëª©", "ê°’"],
            rows=[
                ["S3 ê²½ë¡œ", analysis_results.get("s3_uri", "")],
                [
                    "ë¶„ì„ ê¸°ê°„ (ìš”ì²­)",
                    f"{analysis_results.get('start_time', '')} ~ {analysis_results.get('end_time', '')}",
                ],
                [
                    "ì‹¤ì œ ë°ì´í„° ê¸°ê°„",
                    f"{analysis_results.get('actual_start_time', '')} ~ {analysis_results.get('actual_end_time', '')}",
                ],
                ["íƒ€ì„ì¡´", analysis_results.get("timezone", "")],
                ["ì´ ë¡œê·¸ ë¼ì¸", f"{total_logs:,}"],
                ["ë¡œê·¸ íŒŒì¼ ìˆ˜", f"{analysis_results.get('log_files_count') or 0:,}"],
                ["ìˆ˜ì‹  ë°ì´í„°", format_bytes(total_received)],
                ["ì†¡ì‹  ë°ì´í„°", format_bytes(total_sent)],
            ],
            sortable=False,
            searchable=False,
        )

        # íŒŒì¼ëª… ìƒì„± ë° ì €ì¥
        report_filename = _generate_report_filename(analyzer, analysis_results).replace(".xlsx", ".html")
        html_path = os.path.join(output_dir, report_filename)

        report.save(html_path, auto_open=False)
        return html_path

    except Exception as e:
        console.print(f"[yellow]âš ï¸ HTML ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}[/yellow]")
        import traceback

        traceback.print_exc()
        return None


def _generate_report_filename(analyzer, analysis_results: dict[str, Any]) -> str:
    """ë³´ê³ ì„œ íŒŒì¼ëª… ìƒì„±"""
    import secrets

    try:
        # ì‹œê°„ ë²”ìœ„ ì •ë³´
        start_dt = analyzer.start_datetime
        end_dt = analyzer.end_datetime
        time_diff = end_dt - start_dt
        hours = int(time_diff.total_seconds() / 3600)

        if hours < 24:
            pass
        else:
            hours // 24
            hours % 24

        # ê³„ì •/ë¦¬ì „ ì •ë³´
        account_id = "unknown"
        region = "unknown"

        s3_uri = f"s3://{analyzer.bucket_name}/{analyzer.prefix}"
        if "/AWSLogs/" in s3_uri:
            path = s3_uri.replace("s3://", "")
            parts = path.split("/AWSLogs/")[1].split("/")
            if len(parts) >= 3:
                account_id = parts[0]
                region = parts[2]

        # ALB ì´ë¦„
        alb_name = analysis_results.get("alb_name") or "alb"
        alb_name = str(alb_name).strip().replace("/", "-").replace("\\", "-")

        # íŒŒì¼ëª… ìƒì„±
        random_suffix = secrets.token_hex(4)
        return f"{account_id}_{region}_{alb_name}_report_{random_suffix}.xlsx"

    except Exception as e:
        logger.debug("Failed to generate report filename: %s", e)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"ALB_Log_Analysis_{timestamp}.xlsx"
