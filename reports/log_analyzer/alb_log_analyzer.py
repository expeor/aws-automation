#!/usr/bin/env python3
"""
ğŸš€ DuckDB ê¸°ë°˜ ALB ë¡œê·¸ ë¶„ì„ê¸°

ê¸°ì¡´ íŒŒì‹± ë¡œì§ì„ DuckDB SQLë¡œ êµì²´í•˜ì—¬ ì´ˆê³ ì† ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ ì™„ì „ í˜¸í™˜ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import contextlib
import os
import re
import shutil
from datetime import datetime
from pathlib import Path
from typing import Any

import pytz  # type: ignore[import-untyped]

# DuckDB - optional dependency
try:
    import duckdb
except ImportError:
    duckdb = None  # type: ignore

from rich.console import Console
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
)

# ì½˜ì†” ë° ë¡œê±° (aa_cli.aa.ui ë˜ëŠ” ë¡œì»¬ ìƒì„±)
try:
    from cli.ui import console, logger, print_sub_info, print_sub_task_done
except ImportError:
    import logging

    console = Console()
    logger = logging.getLogger(__name__)

    # Fallback functions
    def print_sub_info(message: str) -> None:
        console.print(f"[blue]{message}[/blue]")

    def print_sub_task_done(message: str) -> None:
        console.print(f"[green]âœ“ {message}[/green]")


from core.tools.cache import get_cache_dir

from .alb_log_downloader import ALBLogDownloader
from .ip_intelligence import IPIntelligence


def _check_duckdb():
    """DuckDB ì„¤ì¹˜ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    if duckdb is None:
        raise ImportError(
            "âŒ DuckDBê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
            "   ALB ë¡œê·¸ ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:\n\n"
            "   pip install duckdb"
        )


class ALBLogAnalyzer:
    """ğŸš€ DuckDB ê¸°ë°˜ ALB ë¡œê·¸ë¥¼ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""

    def __init__(
        self,
        s3_client: Any,
        bucket_name: str,
        prefix: str,
        start_datetime: Any,
        end_datetime: Any | None = None,
        timezone: str = "Asia/Seoul",
        max_workers: int = 5,
    ):
        """ALB ë¡œê·¸ ë¶„ì„ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        # DuckDB ì„¤ì¹˜ í™•ì¸
        _check_duckdb()

        self.s3_client = s3_client
        self.bucket_name = bucket_name
        self.prefix = prefix.strip("/")

        # datetime ê°ì²´ ë˜ëŠ” ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
        if isinstance(start_datetime, str):
            try:
                self.start_datetime = datetime.strptime(start_datetime, "%Y-%m-%d %H:%M")
            except ValueError as e:
                raise ValueError(f"ì˜ëª»ëœ ì‹œì‘ ì‹œê°„ í˜•ì‹: {start_datetime}") from e
        else:
            self.start_datetime = start_datetime

        if end_datetime is None:
            self.end_datetime = datetime.now()
        elif isinstance(end_datetime, str):
            try:
                self.end_datetime = datetime.strptime(end_datetime, "%Y-%m-%d %H:%M")
            except ValueError as e:
                raise ValueError(f"ì˜ëª»ëœ ì¢…ë£Œ ì‹œê°„ í˜•ì‹: {end_datetime}") from e
        else:
            self.end_datetime = end_datetime

        # íƒ€ì„ì¡´ ì„¤ì •
        try:
            self.timezone = pytz.timezone(timezone)
        except pytz.exceptions.UnknownTimeZoneError:
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´ '{timezone}'ì…ë‹ˆë‹¤. UTCë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.timezone = pytz.UTC

        self.console = console
        self.max_workers = max_workers

        # ALBLogDownloader ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.downloader = ALBLogDownloader(
            s3_client=s3_client,
            s3_uri=f"s3://{bucket_name}/{prefix}",
            start_datetime=start_datetime,
            end_datetime=end_datetime,
            timezone=timezone,
            max_workers=max_workers,
        )

        # ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì • (temp/alb í•˜ìœ„)
        self.base_dir = get_cache_dir("alb")
        self.temp_dir = os.path.join(self.base_dir, "gz")
        self.decompressed_dir = os.path.join(self.base_dir, "log")
        self.download_dir = self.temp_dir

        # DuckDB ì„ì‹œ/ë°ì´í„° ë””ë ‰í† ë¦¬
        self.temp_work_dir = os.getenv("AA_DUCKDB_TEMP_DIR") or os.path.join(self.base_dir, "duckdb")
        self.duckdb_dir = os.path.join(self.base_dir, "checkpoint")
        self.duckdb_db_path = os.path.join(self.duckdb_dir, "alb_logs.duckdb")

        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.temp_dir, exist_ok=True)
        os.makedirs(self.decompressed_dir, exist_ok=True)
        os.makedirs(self.temp_work_dir, exist_ok=True)
        os.makedirs(self.duckdb_dir, exist_ok=True)

        # ğŸš€ DuckDB ì—°ê²° ì´ˆê¸°í™” (íŒŒì¼ DBë¡œ ì „í™˜)
        self.conn = duckdb.connect(self.duckdb_db_path, read_only=False)
        self._setup_duckdb()

        # ğŸŒ IP ì¸í…”ë¦¬ì „ìŠ¤ ì´ˆê¸°í™” (êµ­ê°€ ë§¤í•‘ + ì•…ì„± IP)
        self.ip_intel = IPIntelligence()

    def _setup_duckdb(self):
        """DuckDB ì„¤ì • ë° ALB ë¡œê·¸ íŒŒì‹± í•¨ìˆ˜ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # DuckDB ì„¤ì • ìµœì í™” (í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥)
            memory_limit = os.getenv("AA_DUCKDB_MEMORY_LIMIT", "2GB")
            threads_default = min(8, os.cpu_count() or 8)
            try:
                threads = int(os.getenv("AA_DUCKDB_THREADS", str(threads_default)))
            except ValueError:
                threads = threads_default

            temp_dir_sql = Path(self.temp_work_dir).as_posix()

            self.conn.execute(f"SET temp_directory='{temp_dir_sql}'")
            self.conn.execute(f"SET memory_limit='{memory_limit}'")
            self.conn.execute(f"SET threads={threads}")
            self.conn.execute("SET enable_progress_bar=false")

            # ALB ë¡œê·¸ íŒŒì‹±ì„ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë“¤
            self._create_alb_parsing_functions()

            logger.debug("âœ… DuckDB ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ DuckDB ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            raise

    def _create_alb_parsing_functions(self):
        """ALB ë¡œê·¸ íŒŒì‹±ì„ ìœ„í•œ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ìƒì„±"""

        # ê°„ë‹¨í•œ ì •ê·œì‹ ê¸°ë°˜ íŒŒì‹± ë§¤í¬ë¡œë“¤ (DuckDB MACRO)
        # íƒ€ì„ì¡´ ë³€í™˜: ALB ë¡œê·¸ëŠ” UTCë¡œ ê¸°ë¡ë˜ë¯€ë¡œ, ì‚¬ìš©ì íƒ€ì„ì¡´ìœ¼ë¡œ ë³€í™˜
        tz_name = self.timezone.zone if hasattr(self.timezone, "zone") else str(self.timezone)
        functions = [
            # UTC íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ íŒŒì‹± í›„ ì‚¬ìš©ì íƒ€ì„ì¡´ìœ¼ë¡œ ë³€í™˜
            f"""CREATE OR REPLACE MACRO extract_timestamp(log_line) AS (
                   timezone('{tz_name}',
                       strptime(regexp_extract(log_line, '\\S+ (\\S+) ', 1), '%Y-%m-%dT%H:%M:%S.%fZ')
                       AT TIME ZONE 'UTC'
                   )
               )""",
            """CREATE OR REPLACE MACRO extract_client_ip(log_line) AS (
                   split_part(regexp_extract(log_line, '\\S+ \\S+ \\S+ (\\S+) ', 1), ':', 1)
               )""",
            """CREATE OR REPLACE MACRO extract_target_ip(log_line) AS (
                   CASE
                       WHEN regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) = '-' THEN ''
                       ELSE split_part(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ (\\S+) ', 1), ':', 1)
                   END
               )""",
            """CREATE OR REPLACE MACRO extract_elb_status(log_line) AS (
                   regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1)
               )""",
            """CREATE OR REPLACE MACRO extract_target_status(log_line) AS (
                   regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1)
               )""",
            """CREATE OR REPLACE MACRO extract_response_time(log_line) AS (
                   CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS DOUBLE) +
                   CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS DOUBLE) +
                   CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS DOUBLE)
               )""",
            """CREATE OR REPLACE MACRO extract_request(log_line) AS (
                   regexp_extract(log_line, '"([^\"]*)"', 1)
               )""",
            """CREATE OR REPLACE MACRO extract_method(log_line) AS (
                   split_part(regexp_extract(log_line, '"([^\"]*)"', 1), ' ', 1)
               )""",
            """CREATE OR REPLACE MACRO extract_url(log_line) AS (
                   split_part(regexp_extract(log_line, '"([^\"]*)"', 1), ' ', 2)
               )""",
            """CREATE OR REPLACE MACRO extract_user_agent(log_line) AS (
                   coalesce(regexp_extract(log_line, '"[^\"]*"\\s+"([^\"]*)"', 1), '')
               )""",
            """CREATE OR REPLACE MACRO extract_received_bytes(log_line) AS (
                   CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS BIGINT)
               )""",
            """CREATE OR REPLACE MACRO extract_sent_bytes(log_line) AS (
                   CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS BIGINT)
               )""",
            # ì¶”ê°€ í•„ë“œ: target_port
            """CREATE OR REPLACE MACRO extract_target_port(log_line) AS (
                   CASE
                       WHEN regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) = '-' THEN ''
                       ELSE split_part(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ (\\S+) ', 1), ':', 2)
                   END
               )""",
            # ì²˜ë¦¬ ì‹œê°„ 3í•„ë“œ ë¶„ë¦¬ (-1ì€ íƒ€ì„ì•„ì›ƒ/ì—°ê²°ì‹¤íŒ¨ë¥¼ ì˜ë¯¸, NULLë¡œ ì²˜ë¦¬)
            """CREATE OR REPLACE MACRO extract_request_proc_time(log_line) AS (
                   CASE WHEN regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) IN ('-', '-1') THEN NULL
                        WHEN CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS DOUBLE) < 0 THEN NULL
                        ELSE CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS DOUBLE) END
               )""",
            """CREATE OR REPLACE MACRO extract_target_proc_time(log_line) AS (
                   CASE WHEN regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) IN ('-', '-1') THEN NULL
                        WHEN CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS DOUBLE) < 0 THEN NULL
                        ELSE CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS DOUBLE) END
               )""",
            """CREATE OR REPLACE MACRO extract_response_proc_time(log_line) AS (
                   CASE WHEN regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) IN ('-', '-1') THEN NULL
                        WHEN CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS DOUBLE) < 0 THEN NULL
                        ELSE CAST(regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) AS DOUBLE) END
               )""",
            # ì´ ì‘ë‹µ ì‹œê°„: ëª¨ë“  í•„ë“œê°€ NULLì´ë©´ NULL, ì•„ë‹ˆë©´ í•©ì‚° (NULLì€ 0ìœ¼ë¡œ ì²˜ë¦¬)
            """CREATE OR REPLACE MACRO extract_total_response_time(log_line) AS (
                   CASE
                       WHEN extract_request_proc_time(log_line) IS NULL
                            AND extract_target_proc_time(log_line) IS NULL
                            AND extract_response_proc_time(log_line) IS NULL
                       THEN NULL
                       ELSE coalesce(extract_request_proc_time(log_line), 0) +
                            coalesce(extract_target_proc_time(log_line), 0) +
                            coalesce(extract_response_proc_time(log_line), 0)
                   END
               )""",
            # target í•„ë“œ (5ë²ˆì§¸ space-separated field, target:port í˜•íƒœ)
            """CREATE OR REPLACE MACRO extract_target(log_line) AS (
                   CASE
                       WHEN regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ (\\S+) ', 1) = '-' THEN ''
                       ELSE regexp_extract(log_line, '\\S+ \\S+ \\S+ \\S+ (\\S+) ', 1)
                   END
               )""",
            # target_group_arn ë° name (ë¼ì¸ ë‚´ ì–´ë””ì„œë“  ì•ˆì „í•˜ê²Œ ì¶”ì¶œ)
            """CREATE OR REPLACE MACRO extract_target_group_arn(log_line) AS (
                   coalesce(regexp_extract(log_line, '(arn:aws:elasticloadbalancing:[^\\s]+:targetgroup/[^\\s]+)', 1), '')
               )""",
            """CREATE OR REPLACE MACRO extract_target_group_name(log_line) AS (
                   coalesce(regexp_extract(log_line, 'targetgroup/([^/]+)/', 1), '')
               )""",
            # redirect_url (ë§ˆì§€ë§‰ 7ê°œ quoted field ì¤‘ ë‘ ë²ˆì§¸)
            """CREATE OR REPLACE MACRO extract_redirect_url(log_line) AS (
                   coalesce(regexp_extract(log_line, '"[^\"]*"\\s+"([^\"]*)"\\s+"[^\"]*"\\s+"[^\"]*"\\s+"[^\"]*"\\s+"[^\"]*"\\s+"[^\"]*"\\s+\\S+\\s*$', 1), '')
               )""",
            # error_reason (ë§ˆì§€ë§‰ 7ê°œ quoted field ì¤‘ ì„¸ ë²ˆì§¸)
            """CREATE OR REPLACE MACRO extract_error_reason(log_line) AS (
                   coalesce(regexp_extract(log_line, '"[^\"]*"\\s+"[^\"]*"\\s+"([^\"]*)"\\s+"[^\"]*"\\s+"[^\"]*"\\s+"[^\"]*"\\s+"[^\"]*"\\s+\\S+\\s*$', 1), '')
               )""",
            # elb ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: app/my-alb-name/50dc6... -> my-alb-name)
            """CREATE OR REPLACE MACRO extract_elb_full(log_line) AS (
                   regexp_extract(log_line, '\\S+ \\S+ (\\S+) ', 1)
               )""",
            """CREATE OR REPLACE MACRO extract_elb_name(log_line) AS (
                   coalesce(regexp_extract(extract_elb_full(log_line), '^[^/]+/([^/]+)/', 1), '')
               )""",
            # ========== ì¶”ê°€ ë¶„ì„ í•„ë“œ (Phase 2) ==========
            # HTTP ë²„ì „ ì¶”ì¶œ (request í•„ë“œì—ì„œ: "GET /path HTTP/1.1")
            """CREATE OR REPLACE MACRO extract_http_version(log_line) AS (
                   CASE
                       WHEN regexp_extract(log_line, '"[^"]*\\s+HTTP/2[^"]*"', 0) IS NOT NULL THEN 'HTTP/2'
                       WHEN regexp_extract(log_line, '"[^"]*\\s+HTTP/1\\.1[^"]*"', 0) IS NOT NULL THEN 'HTTP/1.1'
                       WHEN regexp_extract(log_line, '"[^"]*\\s+HTTP/1\\.0[^"]*"', 0) IS NOT NULL THEN 'HTTP/1.0'
                       WHEN log_line LIKE '%grpc%' OR log_line LIKE '%gRPC%' THEN 'gRPC'
                       WHEN log_line LIKE 'h2 %' OR log_line LIKE 'grpcs %' THEN 'HTTP/2'
                       WHEN log_line LIKE 'wss %' OR log_line LIKE 'ws %' THEN 'WebSocket'
                       ELSE 'Unknown'
                   END
               )""",
            # SSL/TLS í”„ë¡œí† ì½œ (í•„ë“œ 15: ssl_protocol - TLSv1.2, TLSv1.3 ë“±)
            """CREATE OR REPLACE MACRO extract_ssl_protocol(log_line) AS (
                   coalesce(
                       regexp_extract(log_line, '\\s(TLSv1\\.[0-3])\\s', 1),
                       CASE WHEN log_line LIKE 'http %' THEN 'None' ELSE '-' END
                   )
               )""",
            # SSL/TLS ì•”í˜¸ ìŠ¤ìœ„íŠ¸ (í•„ë“œ 14: ssl_cipher)
            """CREATE OR REPLACE MACRO extract_ssl_cipher(log_line) AS (
                   coalesce(
                       regexp_extract(log_line, '\\s([A-Z][A-Z0-9]+-[A-Z0-9-]+)\\s+TLSv', 1),
                       CASE WHEN log_line LIKE 'http %' THEN 'None' ELSE '-' END
                   )
               )""",
            # Actions Executed (í•„ë“œ 22: "waf,forward", "authenticate,forward" ë“±)
            """CREATE OR REPLACE MACRO extract_actions(log_line) AS (
                   coalesce(
                       regexp_extract(log_line, '"(waf[^"]*|forward|redirect|fixed-response|authenticate[^"]*)"', 1),
                       '-'
                   )
               )""",
            # Classification (í•„ë“œ 28: Acceptable, Ambiguous, Severe)
            """CREATE OR REPLACE MACRO extract_classification(log_line) AS (
                   coalesce(
                       regexp_extract(log_line, '"(Acceptable|Ambiguous|Severe)"', 1),
                       'Unknown'
                   )
               )""",
            # Classification Reason (í•„ë“œ 29)
            """CREATE OR REPLACE MACRO extract_classification_reason(log_line) AS (
                   coalesce(
                       regexp_extract(log_line, '"(Acceptable|Ambiguous|Severe)"\\s+"([^"]*)"', 2),
                       '-'
                   )
               )""",
        ]

        # í•¨ìˆ˜ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ ì‹¤í–‰
        for func_sql in functions:
            try:
                self.conn.execute(func_sql)
            except Exception as e:
                logger.debug(f"í•¨ìˆ˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {str(e)}")

    def download_logs(self) -> list[str]:
        """S3ì—ì„œ ë¡œê·¸ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        return self.downloader.download_logs()

    def decompress_logs(self, gz_directory: str) -> str:
        """ì••ì¶•ëœ ë¡œê·¸ íŒŒì¼ì„ í•´ì œí•©ë‹ˆë‹¤."""
        return self.downloader.decompress_logs(gz_directory)

    def analyze_logs(self, log_directory: str) -> dict[str, Any]:
        """DuckDB ê¸°ë°˜ ë¡œê·¸ íŒŒì¼ë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            print_sub_info("ALB ë¡œê·¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

            # ë‹¨ì¼ ì§„í–‰ ë°”ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì§„í–‰ ìƒí™© í‘œì‹œ
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TaskProgressColumn(),
                console=self.console,
            ) as progress:
                task = progress.add_task("[cyan]ë¶„ì„ ì¤‘...", total=7)

                # 1) ë¡œê·¸ íŒŒì¼ë“¤ì„ DuckDBë¡œ ë¡œë“œ
                progress.update(task, description="[cyan]ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì¤‘...")
                table_name = self._load_logs_to_duckdb(log_directory)
                if not table_name:
                    logger.warning("ë¶„ì„í•  ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return self._get_empty_analysis_results()
                progress.advance(task)

                # 2) DuckDBë¡œ ë¡œê·¸ ë¶„ì„ ìˆ˜í–‰ (5ë‹¨ê³„)
                analysis_results = self._analyze_with_duckdb(progress=progress, task_id=task)

            # AbuseIPDB ë°ì´í„° ì¶”ê°€ (IPIntelligence í†µí•© API ì‚¬ìš©)
            progress.update(task, description="[cyan]AbuseIPDB ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
            abuseipdb_result = self.ip_intel.download_abuse_data()

            # AbuseIPDB ê²°ê³¼ì—ì„œ ì‹¤ì œ IP ë¦¬ìŠ¤íŠ¸ì™€ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            abuse_ips_data = abuseipdb_result.get("abuse_ips", [])
            abuse_ip_details = abuseipdb_result.get("abuse_ip_details", {})

            # abuse_ips_dataê°€ setì¸ ê²½ìš° listë¡œ ë³€í™˜
            if isinstance(abuse_ips_data, set):
                abuse_ips_list = list(abuse_ips_data)
            elif isinstance(abuse_ips_data, list):
                abuse_ips_list = abuse_ips_data
            else:
                abuse_ips_list = []

            # AbuseIPDB ë°ì´í„°ë¥¼ ë¶„ì„ ê²°ê³¼ì— ì¶”ê°€
            analysis_results["abuse_ips"] = abuse_ips_list
            analysis_results["abuse_ips_list"] = abuse_ips_list
            analysis_results["abuse_ip_details"] = abuse_ip_details

            progress.update(task, description="[green]âœ“ ë¶„ì„ ì™„ë£Œ!")
            print_sub_task_done("ALB ë¡œê·¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return analysis_results

        except Exception as e:
            logger.error(f"ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise Exception(f"ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}") from e

    def _load_logs_to_duckdb(self, log_directory: str) -> str | None:
        """ë¡œê·¸ íŒŒì¼ë“¤ì„ DuckDB í…Œì´ë¸”ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
            log_files = []
            for root, _, files in os.walk(log_directory):
                for file in files:
                    if file.endswith(".log"):
                        log_files.append(os.path.join(root, file))

            if not log_files:
                logger.warning("íŒŒì‹±í•  ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None

            logger.debug(f"ğŸ“ {len(log_files)}ê°œì˜ ë¡œê·¸ íŒŒì¼ ë°œê²¬")

            # ê° ë‚ ì§œë³„ íŒŒì¼ ìˆ˜ ê³„ì‚° - íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
            date_counts: dict[str, int] = {}
            for log_file in log_files:
                # 1) íŒŒì¼ ê²½ë¡œì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹)
                date_match = re.search(r"(\d{4})[/\\](\d{2})[/\\](\d{2})", log_file)
                if date_match:
                    date_str = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}"
                    date_counts[date_str] = date_counts.get(date_str, 0) + 1
                else:
                    # 2) íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„ (ALB ë¡œê·¸ íŒŒì¼ëª… í˜•ì‹)
                    filename = os.path.basename(log_file)
                    # íŒŒì¼ëª…: account_elasticloadbalancing_region_loadbalancer_20250817T000000Z_ip_random.log
                    # ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
                    timestamp_patterns = [
                        r"_(\d{8})T\d{6}Z?_",  # _20250817T123456Z_
                        r"_(\d{8})T\d{6}_",  # _20250817T123456_
                        r"_(\d{4}-\d{2}-\d{2})T",  # _2025-08-17T
                        r"(\d{8})T\d{6}",  # 20250817T123456
                        r"(\d{4}\d{2}\d{2})_\d{6}_",  # 20250817_123456_
                    ]

                    timestamp_match = None
                    for pattern in timestamp_patterns:
                        timestamp_match = re.search(pattern, filename)
                        if timestamp_match:
                            break
                    if timestamp_match:
                        date_part = timestamp_match.group(1)  # 20250817 ë˜ëŠ” 2025-08-17
                        if "-" in date_part:
                            date_str = date_part  # ì´ë¯¸ YYYY-MM-DD í˜•ì‹
                        else:
                            date_str = f"{date_part[:4]}-{date_part[4:6]}-{date_part[6:8]}"
                        date_counts[date_str] = date_counts.get(date_str, 0) + 1
                    else:
                        # 3) ì¶”ê°€ íŒ¨í„´ ì‹œë„ - íŒŒì¼ëª… ì „ì²´ì—ì„œ ë‚ ì§œ ì°¾ê¸°
                        date_anywhere = re.search(r"(\d{4}[\-_]?\d{2}[\-_]?\d{2})", filename)
                        if date_anywhere:
                            raw_date = date_anywhere.group(1).replace("_", "-")
                            if len(raw_date) == 8:  # YYYYMMDD
                                date_str = f"{raw_date[:4]}-{raw_date[4:6]}-{raw_date[6:8]}"
                            else:
                                date_str = raw_date
                            date_counts[date_str] = date_counts.get(date_str, 0) + 1
                        else:
                            # ë””ë²„ê¹…ì„ ìœ„í•´ íŒŒì¼ëª… ì˜ˆì‹œ ì¶œë ¥
                            if date_counts.get("unknown", 0) < 3:
                                logger.debug(f"ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨ íŒŒì¼ëª… ì˜ˆì‹œ: {filename}")
                            date_counts["unknown"] = date_counts.get("unknown", 0) + 1

            if date_counts:
                logger.debug(f"ğŸ“… ë‚ ì§œë³„ íŒŒì¼ ë¶„í¬: {date_counts}")
                # ì •ë ¬ëœ ë‚ ì§œë¡œ í‘œì‹œ
                sorted_dates = sorted([k for k in date_counts if k != "unknown"])
                if sorted_dates:
                    logger.debug(f"ğŸ“Š ë‚ ì§œ ë²”ìœ„: {sorted_dates[0]} ~ {sorted_dates[-1]}")

            # ë¡œë“œëœ íŒŒì¼ ë©”íƒ€ ì €ì¥ (Summary ì‹œíŠ¸ í‘œì‹œìš©)
            try:
                self.loaded_log_files_count = len(log_files)
                self.loaded_log_files_paths = log_files
                self.loaded_log_directory = log_directory
            except Exception:
                pass  # nosec B110 - Non-critical metadata assignment

            # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ DuckDBê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë¦¬ìŠ¤íŠ¸ ë¦¬í„°ëŸ´ë¡œ ë³€í™˜
            backslash = "\\"
            file_list_sql = ", ".join([f"'{p.replace(backslash, '/')}'" for p in log_files])

            # ë¡œê·¸ íŒŒì¼ë“¤ì„ í•˜ë‚˜ì˜ í…Œì´ë¸”ë¡œ ë¡œë“œ
            create_table_query = f"""
            CREATE OR REPLACE TABLE alb_logs AS
            SELECT
                line as raw_line,
                extract_timestamp(line) as timestamp,
                extract_client_ip(line) as client_ip,
                extract_target_ip(line) as target_ip,
                extract_target_port(line) as target_port,
                extract_target(line) as target,
                extract_elb_full(line) as elb_full,
                extract_elb_name(line) as elb_name,
                extract_elb_status(line) as elb_status_code,
                extract_target_status(line) as target_status_code,
                extract_request_proc_time(line) as request_processing_time,
                extract_target_proc_time(line) as target_processing_time,
                extract_response_proc_time(line) as response_processing_time,
                extract_total_response_time(line) as response_time,
                extract_request(line) as request,
                extract_method(line) as http_method,
                extract_url(line) as url,
                extract_user_agent(line) as user_agent,
                extract_target_group_arn(line) as target_group_arn,
                extract_target_group_name(line) as target_group_name,
                extract_redirect_url(line) as redirect_url,
                extract_error_reason(line) as error_reason,
                extract_received_bytes(line) as received_bytes,
                extract_sent_bytes(line) as sent_bytes,
                -- ì¶”ê°€ ë¶„ì„ í•„ë“œ (Phase 2)
                extract_http_version(line) as http_version,
                extract_ssl_protocol(line) as ssl_protocol,
                extract_ssl_cipher(line) as ssl_cipher,
                extract_actions(line) as actions_executed,
                extract_classification(line) as classification,
                extract_classification_reason(line) as classification_reason
            FROM read_csv_auto([{file_list_sql}],
                              delim='\\t',
                              header=false,
                              columns={{'line': 'VARCHAR'}},
                              ignore_errors=true)
            WHERE line IS NOT NULL
              AND line != ''
              AND length(line) > 50
            """

            # ë¡œê·¸ ë¡œë“œ ë° ì²´í¬í¬ì¸íŠ¸ (ìƒìœ„ Progressì—ì„œ ê´€ë¦¬)
            self.conn.execute(create_table_query)
            # ë¡œë“œ ì§í›„ ë””ìŠ¤í¬ì— í”ŒëŸ¬ì‹œí•˜ì—¬ ë©”ëª¨ë¦¬ ì••ë°•ì„ ì¤„ì„
            with contextlib.suppress(Exception):
                self.conn.execute("CHECKPOINT")

            # ë¡œë“œëœ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
            count_result = self.conn.execute("SELECT COUNT(*) FROM alb_logs").fetchone()
            total_records = count_result[0] if count_result else 0

            logger.debug(f"âœ… ì´ {total_records:,}ê°œì˜ ë¡œê·¸ ë ˆì½”ë“œ ë¡œë“œ ì™„ë£Œ")

            return "alb_logs"

        except Exception as e:
            logger.error(f"âŒ ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return None

    def _analyze_with_duckdb(
        self,
        progress: Progress | None = None,
        task_id: Any | None = None,
    ) -> dict[str, Any]:
        """DuckDBë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            # ğŸ¯ íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ì´ë¯¸ ì‚¬ìš©ì íƒ€ì„ì¡´ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ì €ì¥ë˜ë¯€ë¡œ
            # í•„í„°ë§ë„ ì‚¬ìš©ì íƒ€ì„ì¡´ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜í–‰
            start_local = self.start_datetime.strftime("%Y-%m-%d %H:%M:%S")
            end_local = self.end_datetime.strftime("%Y-%m-%d %H:%M:%S")

            summary_query = f"""
            SELECT
                COUNT(*) as total_logs,
                COUNT(DISTINCT client_ip) as unique_client_ips,
                MIN(timestamp) as start_time,
                MAX(timestamp) as end_time,
                AVG(response_time) as avg_response_time,
                SUM(received_bytes) as total_received_bytes,
                SUM(sent_bytes) as total_sent_bytes,
                SUM(CASE WHEN elb_status_code LIKE '2%' AND elb_status_code != '-' AND elb_status_code IS NOT NULL THEN 1 ELSE 0 END) as elb_2xx_count,
                SUM(CASE WHEN elb_status_code LIKE '3%' AND elb_status_code != '-' AND elb_status_code IS NOT NULL THEN 1 ELSE 0 END) as elb_3xx_count,
                SUM(CASE WHEN elb_status_code LIKE '4%' AND elb_status_code != '-' AND elb_status_code IS NOT NULL THEN 1 ELSE 0 END) as elb_4xx_count,
                SUM(CASE WHEN elb_status_code LIKE '5%' AND elb_status_code != '-' AND elb_status_code IS NOT NULL THEN 1 ELSE 0 END) as elb_5xx_count,
                SUM(CASE WHEN target_status_code LIKE '4%' AND target_status_code != '-' AND target_status_code IS NOT NULL THEN 1 ELSE 0 END) as backend_4xx_count,
                SUM(CASE WHEN target_status_code LIKE '5%' AND target_status_code != '-' AND target_status_code IS NOT NULL THEN 1 ELSE 0 END) as backend_5xx_count
            FROM alb_logs
            WHERE timestamp IS NOT NULL
              AND timestamp >= '{start_local}'
              AND timestamp <= '{end_local}'
            """

            # 1) ìš”ì•½ í†µê³„
            if progress is not None and task_id is not None:
                progress.update(task_id, description="[cyan]ìš”ì•½ í†µê³„ ê³„ì‚° ì¤‘...")
            summary_result = self.conn.execute(summary_query).fetchone()
            if summary_result is None:
                raise ValueError("Failed to get summary statistics from database")
            if progress is not None and task_id is not None:
                progress.advance(task_id)

            # 2) ì¹´ìš´íŠ¸ ê³„ì‚°
            if progress is not None and task_id is not None:
                progress.update(task_id, description="[cyan]IP/URL/User Agent ì¹´ìš´íŠ¸ ì¤‘...")
            client_ip_query = """
            SELECT client_ip, COUNT(*) as count
            FROM alb_logs
            WHERE client_ip != '' AND client_ip IS NOT NULL
            GROUP BY client_ip
            ORDER BY count DESC
            """
            client_ip_results = self.conn.execute(client_ip_query).fetchall()
            client_ip_counts = {ip: count for ip, count in client_ip_results}

            # Clientë³„ ìƒíƒœì½”ë“œ í†µê³„
            client_status_query = """
            SELECT client_ip, elb_status_code, COUNT(*) as count
            FROM alb_logs
            WHERE client_ip != '' AND client_ip IS NOT NULL
              AND elb_status_code IS NOT NULL AND elb_status_code != '-'
            GROUP BY client_ip, elb_status_code
            ORDER BY client_ip, elb_status_code
            """
            client_status_results = self.conn.execute(client_status_query).fetchall()
            client_status_statistics: dict[str, dict[str, int]] = {}
            for client_ip, status_code, count in client_status_results:
                if client_ip not in client_status_statistics:
                    client_status_statistics[client_ip] = {}
                client_status_statistics[client_ip][status_code] = count

            # Targetë³„ ìƒíƒœì½”ë“œ í†µê³„ (targetì´ ìˆëŠ” ê²½ìš°ë§Œ)
            target_status_query = """
            SELECT target, target_group_name, target_group_arn, elb_status_code, target_status_code, COUNT(*) as count
            FROM alb_logs
            WHERE target != '' AND target IS NOT NULL
              AND (
                (elb_status_code IS NOT NULL AND elb_status_code != '-') OR
                (target_status_code IS NOT NULL AND target_status_code != '-')
              )
            GROUP BY target, target_group_name, target_group_arn, elb_status_code, target_status_code
            ORDER BY target, target_group_name, elb_status_code, target_status_code
            """
            target_status_results = self.conn.execute(target_status_query).fetchall()
            target_status_statistics: dict[str, Any] = {}
            for (
                target,
                target_group_name,
                _target_group_arn,
                elb_status,
                target_status,
                count,
            ) in target_status_results:
                # target í‘œì‹œìš© í‚¤ ìƒì„± (ë‹¤ë¥¸ ì‹œíŠ¸ë“¤ê³¼ ë™ì¼í•œ í˜•íƒœ)
                if target and target != "-":
                    target_display_key = f"{target_group_name}({target})" if target_group_name else target
                else:
                    continue  # targetì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ

                if target_display_key not in target_status_statistics:
                    target_status_statistics[target_display_key] = {}

                # ELB ìƒíƒœì½”ë“œ ì²˜ë¦¬
                if elb_status and elb_status != "-":
                    elb_key = f"ELB:{elb_status}"
                    if elb_key not in target_status_statistics[target_display_key]:
                        target_status_statistics[target_display_key][elb_key] = 0
                    target_status_statistics[target_display_key][elb_key] += count

                # Backend ìƒíƒœì½”ë“œ ì²˜ë¦¬ (Targetì—ì„œ ì‹¤ì œ ì‘ë‹µí•œ ê²½ìš°ë§Œ)
                if target_status and target_status != "-":
                    backend_key = f"Backend:{target_status}"
                    if backend_key not in target_status_statistics[target_display_key]:
                        target_status_statistics[target_display_key][backend_key] = 0
                    target_status_statistics[target_display_key][backend_key] += count

            # ìš”ì²­ URL ì¹´ìš´íŠ¸
            request_url_query = """
            SELECT TRIM(url) as url, COUNT(*) as count
            FROM alb_logs
            WHERE url IS NOT NULL AND TRIM(url) != ''
            GROUP BY url
            ORDER BY count DESC
            """
            request_url_results = self.conn.execute(request_url_query).fetchall()
            request_url_counts = {url: count for url, count in request_url_results}

            # User Agent ì¹´ìš´íŠ¸
            user_agent_query = """
            SELECT user_agent, COUNT(*) as count
            FROM alb_logs
            WHERE user_agent != '' AND user_agent IS NOT NULL
            GROUP BY user_agent
            ORDER BY count DESC
            """
            user_agent_results = self.conn.execute(user_agent_query).fetchall()
            user_agent_counts = {ua: count for ua, count in user_agent_results}
            if progress is not None and task_id is not None:
                progress.update(task_id, description="[cyan]IP/URL/User Agent ì¹´ìš´íŠ¸ ì™„ë£Œ...")
                progress.advance(task_id)

            # URL ë³„ ìƒì„¸ í†µê³„ (Top 100 URL ëŒ€ìƒ)
            request_url_details: dict[str, dict[str, Any]] = {}
            try:
                top_urls = [str(url).strip() for url, _ in request_url_results[:100] if url]
                if top_urls:
                    # DuckDB IN ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (quote escape ì²˜ë¦¬)
                    def _escape_sql(val: str) -> str:
                        return val.replace("'", "''")

                    in_list_sql = ", ".join([f"'{_escape_sql(u)}'" for u in top_urls])

                    # 1) ë©”ì„œë“œë³„ ì¹´ìš´íŠ¸
                    methods_sql = f"""
                    SELECT TRIM(url) as url, TRIM(http_method) as http_method, COUNT(*) as cnt
                    FROM alb_logs
                    WHERE TRIM(url) IN ({in_list_sql}) AND url IS NOT NULL AND TRIM(url) != ''
                    GROUP BY url, http_method
                    """
                    method_rows = self.conn.execute(methods_sql).fetchall()

                    # 2) User-Agentë³„ ì¹´ìš´íŠ¸
                    ua_sql = f"""
                    SELECT TRIM(url) as url, TRIM(user_agent) as user_agent, COUNT(*) as cnt
                    FROM alb_logs
                    WHERE TRIM(url) IN ({in_list_sql}) AND url IS NOT NULL AND TRIM(url) != ''
                    GROUP BY url, user_agent
                    """
                    ua_rows = self.conn.execute(ua_sql).fetchall()

                    # 3) ìƒíƒœì½”ë“œë³„ ì¹´ìš´íŠ¸ (ELB)
                    status_sql = f"""
                    SELECT TRIM(url) as url, elb_status_code, COUNT(*) as cnt
                    FROM alb_logs
                    WHERE TRIM(url) IN ({in_list_sql}) AND url IS NOT NULL AND TRIM(url) != ''
                    GROUP BY url, elb_status_code
                    """
                    status_rows = self.conn.execute(status_sql).fetchall()

                    # 4) ê³ ìœ  IP ìˆ˜
                    unique_ip_sql = f"""
                    SELECT TRIM(url) as url, COUNT(DISTINCT client_ip) as unique_ips
                    FROM alb_logs
                    WHERE TRIM(url) IN ({in_list_sql}) AND url IS NOT NULL AND TRIM(url) != ''
                    GROUP BY url
                    """
                    unique_ip_rows = self.conn.execute(unique_ip_sql).fetchall()

                    # 5) í‰ê·  ì‘ë‹µ ì‹œê°„
                    avg_rt_sql = f"""
                    SELECT TRIM(url) as url, AVG(response_time) as avg_rt
                    FROM alb_logs
                    WHERE TRIM(url) IN ({in_list_sql}) AND url IS NOT NULL AND TRIM(url) != ''
                      AND response_time IS NOT NULL
                    GROUP BY url
                    """
                    avg_rt_rows = self.conn.execute(avg_rt_sql).fetchall()

                    # 6) URLë³„ Top Client IP (ê°€ì¥ ë§ì´ ìš”ì²­í•œ IP)
                    top_client_sql = f"""
                    WITH ranked AS (
                        SELECT TRIM(url) as url, client_ip, COUNT(*) as cnt,
                               ROW_NUMBER() OVER (PARTITION BY TRIM(url) ORDER BY COUNT(*) DESC) as rn
                        FROM alb_logs
                        WHERE TRIM(url) IN ({in_list_sql}) AND url IS NOT NULL AND TRIM(url) != ''
                          AND client_ip IS NOT NULL AND client_ip != ''
                        GROUP BY TRIM(url), client_ip
                    )
                    SELECT url, client_ip FROM ranked WHERE rn = 1
                    """
                    top_client_rows = self.conn.execute(top_client_sql).fetchall()
                    top_client_map = {url: ip for url, ip in top_client_rows}

                    # 7) ì´ ì¹´ìš´íŠ¸ (ì´ë¯¸ ê³„ì‚°ëœ request_url_counts ì‚¬ìš©)
                    for url in top_urls:
                        request_url_details[url] = {
                            "count": int(request_url_counts.get(url, 0) or 0),
                            "methods": {},
                            "user_agents": {},
                            "status_codes": {},
                            "top_client_ip": top_client_map.get(url, ""),
                            # ë©”ëª¨ë¦¬ ì ˆì•½: ì„¸íŠ¸/ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  í†µê³„ ê°’ë§Œ ì €ì¥
                            "unique_ips": 0,
                            "avg_response_time": 0.0,
                        }

                    for url, method, cnt in method_rows:
                        if url in request_url_details:
                            # http_methodê°€ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ëŒ€ì‹œ ì œê±°ì™€ ì¼ì¹˜í•˜ë„ë¡ ì •ê·œí™”ëŠ” ë¦¬í¬í„°ì—ì„œ ì²˜ë¦¬
                            request_url_details[url]["methods"][method] = int(cnt)

                    for url, ua, cnt in ua_rows:
                        if url in request_url_details:
                            request_url_details[url]["user_agents"][ua] = int(cnt)

                    for url, status, cnt in status_rows:
                        if url in request_url_details and status is not None and status != "":
                            request_url_details[url]["status_codes"][status] = int(cnt)

                    for url, uniq in unique_ip_rows:
                        if url in request_url_details:
                            try:
                                request_url_details[url]["unique_ips"] = int(uniq or 0)
                            except Exception:
                                request_url_details[url]["unique_ips"] = 0  # Type conversion fallback

                    for url, avg_rt in avg_rt_rows:
                        if url in request_url_details:
                            try:
                                request_url_details[url]["avg_response_time"] = float(avg_rt or 0.0)
                            except Exception:
                                request_url_details[url]["avg_response_time"] = 0.0  # Type conversion fallback
            except Exception:
                # ì„¸ë¶€ URL í†µê³„ëŠ” ì„ íƒ í•­ëª©ì´ë¯€ë¡œ ì‹¤íŒ¨í•´ë„ ì „ì²´ ë¶„ì„ì„ ê³„ì† (optional stats)
                request_url_details = {}

            # 3) ëŠë¦° ì‘ë‹µ/ë°”ì´íŠ¸ ê³„ì‚°
            if progress is not None and task_id is not None:
                progress.update(task_id, description="[cyan]ëŠë¦° ì‘ë‹µ/ë°”ì´íŠ¸ ë¶„ì„ ì¤‘...")
            long_response_query = """
            SELECT timestamp,
                   client_ip,
                   target_ip,
                   target_port,
                   target,
                   http_method,
                   url,
                   elb_status_code,
                   target_status_code,
                   response_time,
                   received_bytes,
                   sent_bytes,
                   user_agent,
                   target_group_arn,
                   target_group_name
            FROM alb_logs
            ORDER BY response_time DESC
            LIMIT 100
            """
            long_response_results = self.conn.execute(long_response_query).fetchall()
            long_response_times = []
            for row in long_response_results:
                long_response_times.append(
                    {
                        "timestamp": row[0],
                        "client_ip": row[1],
                        "target_ip": row[2],
                        "target_port": row[3],
                        "target": row[4],
                        "http_method": row[5],
                        "request": row[6],
                        "elb_status_code": row[7],
                        "target_status_code": row[8],
                        "response_time": row[9],
                        "received_bytes": row[10],
                        "sent_bytes": row[11],
                        "user_agent": row[12],
                        "target_group_arn": row[13],
                        "target_group_name": row[14],
                    }
                )

            # 1ì´ˆ ì´ìƒ ì‘ë‹µ ì¹´ìš´íŠ¸ (Summaryìš©)
            try:
                long_resp_count_row = self.conn.execute(
                    "SELECT COUNT(*) FROM alb_logs WHERE response_time >= 1.0"
                ).fetchone()
                long_response_count_val = long_resp_count_row[0] if long_resp_count_row else 0
            except Exception:
                long_response_count_val = 0  # Query fallback

            # ì‘ë‹µ ì‹œê°„ ë°±ë¶„ìœ„ìˆ˜ (P50, P90, P95, P99)
            response_time_percentiles: dict[str, float] = {}
            try:
                percentile_query = """
                SELECT
                    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY response_time) as p50,
                    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY response_time) as p90,
                    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY response_time) as p95,
                    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY response_time) as p99,
                    AVG(response_time) as avg,
                    MIN(response_time) as min,
                    MAX(response_time) as max
                FROM alb_logs
                WHERE response_time IS NOT NULL AND response_time >= 0
                """
                percentile_result = self.conn.execute(percentile_query).fetchone()
                if percentile_result:
                    response_time_percentiles = {
                        "p50": float(percentile_result[0] or 0),
                        "p90": float(percentile_result[1] or 0),
                        "p95": float(percentile_result[2] or 0),
                        "p99": float(percentile_result[3] or 0),
                        "avg": float(percentile_result[4] or 0),
                        "min": float(percentile_result[5] or 0),
                        "max": float(percentile_result[6] or 0),
                    }
            except Exception as e:
                logger.debug(f"ì‘ë‹µ ì‹œê°„ ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
                response_time_percentiles = {}

            # ì—ëŸ¬ ì›ì¸(error_reason) ë¶„í¬
            error_reason_counts: dict[str, int] = {}
            try:
                error_reason_query = """
                SELECT error_reason, COUNT(*) as count
                FROM alb_logs
                WHERE error_reason IS NOT NULL
                  AND error_reason != ''
                  AND error_reason != '-'
                GROUP BY error_reason
                ORDER BY count DESC
                """
                error_reason_results = self.conn.execute(error_reason_query).fetchall()
                error_reason_counts = {reason: count for reason, count in error_reason_results if reason}
            except Exception as e:
                logger.debug(f"ì—ëŸ¬ ì›ì¸ ì§‘ê³„ ì‹¤íŒ¨: {e}")
                error_reason_counts = {}

            # Targetë³„ ìš”ì²­ ë¶„í¬ ë° ì—ëŸ¬ìœ¨
            target_request_stats: dict[str, dict[str, Any]] = {}
            try:
                target_stats_query = """
                SELECT
                    target,
                    target_group_name,
                    COUNT(*) as total_requests,
                    SUM(CASE WHEN elb_status_code LIKE '4%' OR elb_status_code LIKE '5%' THEN 1 ELSE 0 END) as error_count,
                    AVG(response_time) as avg_response_time
                FROM alb_logs
                WHERE target IS NOT NULL AND target != '' AND target != '-'
                GROUP BY target, target_group_name
                ORDER BY total_requests DESC
                """
                target_stats_results = self.conn.execute(target_stats_query).fetchall()
                for target, tg_name, total, errors, avg_rt in target_stats_results:
                    display_key = f"{tg_name}({target})" if tg_name else target
                    error_rate = (errors / total * 100) if total > 0 else 0
                    target_request_stats[display_key] = {
                        "total_requests": total,
                        "error_count": errors,
                        "error_rate": round(error_rate, 2),
                        "avg_response_time": round(float(avg_rt or 0), 4),
                    }
            except Exception as e:
                logger.debug(f"Target í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                target_request_stats = {}

            # URLë³„ ì—ëŸ¬ìœ¨
            url_error_stats: dict[str, dict[str, Any]] = {}
            try:
                url_error_query = """
                SELECT
                    TRIM(url) as url,
                    COUNT(*) as total_requests,
                    SUM(CASE WHEN elb_status_code LIKE '4%' THEN 1 ELSE 0 END) as count_4xx,
                    SUM(CASE WHEN elb_status_code LIKE '5%' THEN 1 ELSE 0 END) as count_5xx
                FROM alb_logs
                WHERE url IS NOT NULL AND TRIM(url) != ''
                GROUP BY url
                HAVING COUNT(*) >= 10
                ORDER BY (count_4xx + count_5xx) DESC
                LIMIT 50
                """
                url_error_results = self.conn.execute(url_error_query).fetchall()
                for url, total, c4xx, c5xx in url_error_results:
                    error_total = (c4xx or 0) + (c5xx or 0)
                    error_rate = (error_total / total * 100) if total > 0 else 0
                    url_error_stats[url] = {
                        "total_requests": total,
                        "count_4xx": c4xx or 0,
                        "count_5xx": c5xx or 0,
                        "error_count": error_total,
                        "error_rate": round(error_rate, 2),
                    }
            except Exception as e:
                logger.debug(f"URL ì—ëŸ¬ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
                url_error_stats = {}

            # ë°”ì´íŠ¸ ë¶„ì„
            received_bytes_query = """
            SELECT url, SUM(received_bytes) as total_bytes
            FROM alb_logs
            WHERE received_bytes > 0
            GROUP BY url
            ORDER BY total_bytes DESC
            """
            received_bytes_results = self.conn.execute(received_bytes_query).fetchall()
            received_bytes = {url: bytes_count for url, bytes_count in received_bytes_results}

            sent_bytes_query = """
            SELECT url, SUM(sent_bytes) as total_bytes
            FROM alb_logs
            WHERE sent_bytes > 0
            GROUP BY url
            ORDER BY total_bytes DESC
            """
            sent_bytes_results = self.conn.execute(sent_bytes_query).fetchall()
            sent_bytes = {url: bytes_count for url, bytes_count in sent_bytes_results}

            # ==================================================================================
            # ì„±ëŠ¥ ë¶„ì„ (TPS, ì‹œê°„ë³„ Latency, SLA, Targetë³„ ì„±ëŠ¥)
            # ==================================================================================

            # ì‹œê°„ ë²„í‚· í¬ê¸° ê²°ì • (ë°ì´í„° ë²”ìœ„ì— ë”°ë¼ ì ì‘)
            time_range_seconds = (self.end_datetime - self.start_datetime).total_seconds()
            time_range_hours = time_range_seconds / 3600

            if time_range_hours <= 1:
                bucket_minutes = 1
            elif time_range_hours <= 3:
                bucket_minutes = 5
            elif time_range_hours <= 24:
                bucket_minutes = 15
            elif time_range_hours <= 24 * 7:
                bucket_minutes = 60
            else:
                bucket_minutes = 240

            bucket_interval = f"{bucket_minutes} minutes"

            # ì‹œê°„ ë²„í‚·ë³„ TPS ë° ìš”ì•½ í†µê³„
            tps_time_series: list[dict[str, Any]] = []
            try:
                tps_query = f"""
                SELECT
                    time_bucket(INTERVAL '{bucket_interval}', timestamp) as bucket,
                    COUNT(*) as request_count,
                    ROUND(COUNT(*) / ({bucket_minutes} * 60.0), 2) as tps,
                    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY response_time) as p50,
                    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY response_time) as p90,
                    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY response_time) as p99,
                    SUM(CASE WHEN elb_status_code LIKE '4%' OR elb_status_code LIKE '5%' THEN 1 ELSE 0 END) as error_count
                FROM alb_logs
                WHERE timestamp IS NOT NULL
                  AND timestamp >= '{start_local}'
                  AND timestamp <= '{end_local}'
                GROUP BY bucket
                ORDER BY bucket
                """
                tps_results = self.conn.execute(tps_query).fetchall()
                for bucket_ts, req_count, tps, p50, p90, p99, errors in tps_results:
                    error_rate = (errors / req_count * 100) if req_count > 0 else 0
                    tps_time_series.append(
                        {
                            "timestamp": bucket_ts,
                            "request_count": int(req_count),
                            "tps": float(tps or 0),
                            "p50_ms": round(float(p50 or 0) * 1000, 2),
                            "p90_ms": round(float(p90 or 0) * 1000, 2),
                            "p99_ms": round(float(p99 or 0) * 1000, 2),
                            "error_count": int(errors or 0),
                            "error_rate": round(error_rate, 2),
                        }
                    )
            except Exception as e:
                logger.debug(f"TPS ì‹œê³„ì—´ ê³„ì‚° ì‹¤íŒ¨: {e}")
                tps_time_series = []

            # SLA ì¤€ìˆ˜ìœ¨ ê³„ì‚° (ì‘ë‹µ ì‹œê°„ ì„ê³„ê°’ë³„)
            sla_compliance: dict[str, dict[str, Any]] = {}
            try:
                sla_query = """
                SELECT
                    COUNT(*) as total,
                    SUM(CASE WHEN response_time < 0.1 THEN 1 ELSE 0 END) as under_100ms,
                    SUM(CASE WHEN response_time < 0.5 THEN 1 ELSE 0 END) as under_500ms,
                    SUM(CASE WHEN response_time < 1.0 THEN 1 ELSE 0 END) as under_1s
                FROM alb_logs
                WHERE response_time IS NOT NULL AND response_time >= 0
                """
                sla_result = self.conn.execute(sla_query).fetchone()
                if sla_result and sla_result[0] > 0:
                    total = sla_result[0]
                    sla_compliance = {
                        "under_100ms": {
                            "compliant": int(sla_result[1] or 0),
                            "non_compliant": total - int(sla_result[1] or 0),
                            "rate": round(int(sla_result[1] or 0) / total * 100, 2),
                            "threshold": "< 100ms",
                            "slo_target": 99.0,
                        },
                        "under_500ms": {
                            "compliant": int(sla_result[2] or 0),
                            "non_compliant": total - int(sla_result[2] or 0),
                            "rate": round(int(sla_result[2] or 0) / total * 100, 2),
                            "threshold": "< 500ms",
                            "slo_target": 99.0,
                        },
                        "under_1s": {
                            "compliant": int(sla_result[3] or 0),
                            "non_compliant": total - int(sla_result[3] or 0),
                            "rate": round(int(sla_result[3] or 0) / total * 100, 2),
                            "threshold": "< 1s",
                            "slo_target": 99.9,
                        },
                    }
            except Exception as e:
                logger.debug(f"SLA ì¤€ìˆ˜ìœ¨ ê³„ì‚° ì‹¤íŒ¨: {e}")
                sla_compliance = {}

            # Targetë³„ Latency ë°±ë¶„ìœ„ìˆ˜
            target_latency_stats: dict[str, dict[str, Any]] = {}
            try:
                target_latency_query = """
                SELECT
                    target,
                    target_group_name,
                    COUNT(*) as total_requests,
                    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY response_time) as p50,
                    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY response_time) as p90,
                    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY response_time) as p99,
                    AVG(response_time) as avg_rt,
                    SUM(CASE WHEN elb_status_code LIKE '4%' OR elb_status_code LIKE '5%' THEN 1 ELSE 0 END) as error_count
                FROM alb_logs
                WHERE target IS NOT NULL AND target != '' AND target != '-'
                  AND response_time IS NOT NULL AND response_time >= 0
                GROUP BY target, target_group_name
                ORDER BY total_requests DESC
                LIMIT 50
                """
                target_latency_results = self.conn.execute(target_latency_query).fetchall()
                for target, tg_name, total, p50, p90, p99, avg_rt, errors in target_latency_results:
                    display_key = f"{tg_name}({target})" if tg_name else target
                    error_rate = (errors / total * 100) if total > 0 else 0
                    target_latency_stats[display_key] = {
                        "total_requests": int(total),
                        "p50_ms": round(float(p50 or 0) * 1000, 2),
                        "p90_ms": round(float(p90 or 0) * 1000, 2),
                        "p99_ms": round(float(p99 or 0) * 1000, 2),
                        "avg_ms": round(float(avg_rt or 0) * 1000, 2),
                        "error_count": int(errors or 0),
                        "error_rate": round(error_rate, 2),
                    }
            except Exception as e:
                logger.debug(f"Target Latency í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                target_latency_stats = {}

            # ì‘ë‹µ ì‹œê°„ êµ¬ê°„ë³„ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨ ë°ì´í„°)
            response_time_distribution: dict[str, int] = {}
            try:
                distribution_query = """
                SELECT
                    SUM(CASE WHEN response_time < 0.1 THEN 1 ELSE 0 END) as under_100ms,
                    SUM(CASE WHEN response_time >= 0.1 AND response_time < 0.5 THEN 1 ELSE 0 END) as ms_100_500,
                    SUM(CASE WHEN response_time >= 0.5 AND response_time < 1.0 THEN 1 ELSE 0 END) as ms_500_1000,
                    SUM(CASE WHEN response_time >= 1.0 AND response_time < 3.0 THEN 1 ELSE 0 END) as s_1_3,
                    SUM(CASE WHEN response_time >= 3.0 THEN 1 ELSE 0 END) as over_3s
                FROM alb_logs
                WHERE response_time IS NOT NULL AND response_time >= 0
                """
                dist_result = self.conn.execute(distribution_query).fetchone()
                if dist_result:
                    response_time_distribution = {
                        "<100ms": int(dist_result[0] or 0),
                        "100-500ms": int(dist_result[1] or 0),
                        "500ms-1s": int(dist_result[2] or 0),
                        "1-3s": int(dist_result[3] or 0),
                        ">3s": int(dist_result[4] or 0),
                    }
            except Exception as e:
                logger.debug(f"ì‘ë‹µ ì‹œê°„ ë¶„í¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
                response_time_distribution = {}

            # ==================================================================================
            # ì¶”ê°€ ë¶„ì„ (Phase 2): ì²˜ë¦¬ ì‹œê°„ ë¶„í•´, ì—°ê²° ì‹¤íŒ¨, HTTP ë²„ì „, SSL/TLS, Actions, Classification
            # ==================================================================================

            # ì²˜ë¦¬ ì‹œê°„ ë¶„í•´ ë¶„ì„ (Request/Target/Response ê°ê°ì˜ P50/P90/P99)
            processing_time_breakdown: dict[str, dict[str, float]] = {}
            try:
                breakdown_query = """
                SELECT
                    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY request_processing_time) as req_p50,
                    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY request_processing_time) as req_p90,
                    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY request_processing_time) as req_p99,
                    AVG(request_processing_time) as req_avg,
                    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY target_processing_time) as target_p50,
                    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY target_processing_time) as target_p90,
                    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY target_processing_time) as target_p99,
                    AVG(target_processing_time) as target_avg,
                    PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY response_processing_time) as resp_p50,
                    PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY response_processing_time) as resp_p90,
                    PERCENTILE_CONT(0.99) WITHIN GROUP (ORDER BY response_processing_time) as resp_p99,
                    AVG(response_processing_time) as resp_avg
                FROM alb_logs
                WHERE request_processing_time IS NOT NULL
                  AND target_processing_time IS NOT NULL
                  AND response_processing_time IS NOT NULL
                """
                breakdown_result = self.conn.execute(breakdown_query).fetchone()
                if breakdown_result:
                    processing_time_breakdown = {
                        "request": {
                            "p50_ms": round(float(breakdown_result[0] or 0) * 1000, 3),
                            "p90_ms": round(float(breakdown_result[1] or 0) * 1000, 3),
                            "p99_ms": round(float(breakdown_result[2] or 0) * 1000, 3),
                            "avg_ms": round(float(breakdown_result[3] or 0) * 1000, 3),
                        },
                        "target": {
                            "p50_ms": round(float(breakdown_result[4] or 0) * 1000, 3),
                            "p90_ms": round(float(breakdown_result[5] or 0) * 1000, 3),
                            "p99_ms": round(float(breakdown_result[6] or 0) * 1000, 3),
                            "avg_ms": round(float(breakdown_result[7] or 0) * 1000, 3),
                        },
                        "response": {
                            "p50_ms": round(float(breakdown_result[8] or 0) * 1000, 3),
                            "p90_ms": round(float(breakdown_result[9] or 0) * 1000, 3),
                            "p99_ms": round(float(breakdown_result[10] or 0) * 1000, 3),
                            "avg_ms": round(float(breakdown_result[11] or 0) * 1000, 3),
                        },
                    }
            except Exception as e:
                logger.debug(f"ì²˜ë¦¬ ì‹œê°„ ë¶„í•´ ë¶„ì„ ì‹¤íŒ¨: {e}")
                processing_time_breakdown = {}

            # ì—°ê²° ì‹¤íŒ¨ ë¶„ì„ (-1 ê°’ íƒì§€)
            connection_failures: dict[str, Any] = {}
            try:
                failure_query = """
                SELECT
                    COUNT(*) as total,
                    SUM(CASE WHEN request_processing_time IS NULL THEN 1 ELSE 0 END) as request_failures,
                    SUM(CASE WHEN target_processing_time IS NULL THEN 1 ELSE 0 END) as target_failures,
                    SUM(CASE WHEN response_processing_time IS NULL THEN 1 ELSE 0 END) as response_failures
                FROM alb_logs
                """
                failure_result = self.conn.execute(failure_query).fetchone()
                if failure_result and failure_result[0] > 0:
                    total = failure_result[0]
                    connection_failures = {
                        "total_requests": int(total),
                        "request_failures": int(failure_result[1] or 0),
                        "target_failures": int(failure_result[2] or 0),
                        "response_failures": int(failure_result[3] or 0),
                        "request_failure_rate": round((failure_result[1] or 0) / total * 100, 2),
                        "target_failure_rate": round((failure_result[2] or 0) / total * 100, 2),
                        "response_failure_rate": round((failure_result[3] or 0) / total * 100, 2),
                    }

                # Targetë³„ ì—°ê²° ì‹¤íŒ¨ ìƒì„¸
                target_failure_query = """
                SELECT target, target_group_name, COUNT(*) as failure_count
                FROM alb_logs
                WHERE target_processing_time IS NULL
                  AND target IS NOT NULL AND target != '' AND target != '-'
                GROUP BY target, target_group_name
                ORDER BY failure_count DESC
                LIMIT 20
                """
                target_failure_results = self.conn.execute(target_failure_query).fetchall()
                connection_failures["target_failures_detail"] = [
                    {"target": t, "target_group": tg or "", "count": c} for t, tg, c in target_failure_results
                ]
            except Exception as e:
                logger.debug(f"ì—°ê²° ì‹¤íŒ¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
                connection_failures = {}

            # HTTP ë²„ì „ ë¶„í¬
            http_version_distribution: dict[str, int] = {}
            try:
                http_version_query = """
                SELECT http_version, COUNT(*) as count
                FROM alb_logs
                WHERE http_version IS NOT NULL AND http_version != 'Unknown'
                GROUP BY http_version
                ORDER BY count DESC
                """
                http_version_results = self.conn.execute(http_version_query).fetchall()
                http_version_distribution = {version: int(count) for version, count in http_version_results}
            except Exception as e:
                logger.debug(f"HTTP ë²„ì „ ë¶„í¬ ê³„ì‚° ì‹¤íŒ¨: {e}")
                http_version_distribution = {}

            # SSL/TLS í†µê³„
            ssl_stats: dict[str, Any] = {}
            try:
                # TLS í”„ë¡œí† ì½œ ë¶„í¬
                ssl_protocol_query = """
                SELECT ssl_protocol, COUNT(*) as count
                FROM alb_logs
                WHERE ssl_protocol IS NOT NULL AND ssl_protocol != '-' AND ssl_protocol != 'None'
                GROUP BY ssl_protocol
                ORDER BY count DESC
                """
                ssl_protocol_results = self.conn.execute(ssl_protocol_query).fetchall()
                ssl_stats["protocol_distribution"] = {proto: int(count) for proto, count in ssl_protocol_results}

                # ì•”í˜¸ ìŠ¤ìœ„íŠ¸ Top 10
                ssl_cipher_query = """
                SELECT ssl_cipher, COUNT(*) as count
                FROM alb_logs
                WHERE ssl_cipher IS NOT NULL AND ssl_cipher != '-' AND ssl_cipher != 'None'
                GROUP BY ssl_cipher
                ORDER BY count DESC
                LIMIT 10
                """
                ssl_cipher_results = self.conn.execute(ssl_cipher_query).fetchall()
                ssl_stats["cipher_distribution"] = {cipher: int(count) for cipher, count in ssl_cipher_results}

                # ì·¨ì•½ TLS ë²„ì „ ì‚¬ìš©ì (TLSv1.0, TLSv1.1)
                weak_tls_query = """
                SELECT client_ip, ssl_protocol, COUNT(*) as count
                FROM alb_logs
                WHERE ssl_protocol IN ('TLSv1.0', 'TLSv1.1')
                GROUP BY client_ip, ssl_protocol
                ORDER BY count DESC
                LIMIT 50
                """
                weak_tls_results = self.conn.execute(weak_tls_query).fetchall()
                ssl_stats["weak_tls_clients"] = [
                    {"client_ip": ip, "protocol": proto, "count": int(count)} for ip, proto, count in weak_tls_results
                ]
            except Exception as e:
                logger.debug(f"SSL/TLS í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                ssl_stats = {}

            # Actions í†µê³„
            actions_stats: dict[str, int] = {}
            try:
                actions_query = """
                SELECT
                    CASE
                        WHEN actions_executed LIKE '%waf-failed%' THEN 'WAF Blocked'
                        WHEN actions_executed LIKE '%waf%' THEN 'WAF Passed'
                        WHEN actions_executed LIKE '%authenticate%' THEN 'Authenticated'
                        WHEN actions_executed LIKE '%redirect%' THEN 'Redirect'
                        WHEN actions_executed LIKE '%fixed-response%' THEN 'Fixed Response'
                        WHEN actions_executed LIKE '%forward%' THEN 'Forward'
                        WHEN actions_executed = '-' OR actions_executed IS NULL THEN 'None'
                        ELSE 'Other'
                    END as action_type,
                    COUNT(*) as count
                FROM alb_logs
                GROUP BY action_type
                ORDER BY count DESC
                """
                actions_results = self.conn.execute(actions_query).fetchall()
                actions_stats = {action: int(count) for action, count in actions_results}
            except Exception as e:
                logger.debug(f"Actions í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                actions_stats = {}

            # Classification í†µê³„ (Desync íƒì§€)
            classification_stats: dict[str, Any] = {}
            try:
                classification_query = """
                SELECT classification, COUNT(*) as count
                FROM alb_logs
                WHERE classification IS NOT NULL AND classification != 'Unknown'
                GROUP BY classification
                ORDER BY count DESC
                """
                classification_results = self.conn.execute(classification_query).fetchall()
                classification_stats["distribution"] = {cls: int(count) for cls, count in classification_results}

                # Ambiguous/Severe ìš”ì²­ ìƒì„¸ (ë³´ì•ˆ ì´ë²¤íŠ¸) - Excel ìµœëŒ€ í–‰ ì œí•œ ì ìš©
                # Excel 2007+ (.xlsx) max rows: 1,048,576 (header 1í–‰ ì œì™¸ = 1,048,575)
                security_events_query = f"""
                SELECT timestamp, client_ip, classification, classification_reason, url, elb_status_code
                FROM alb_logs
                WHERE classification IN ('Ambiguous', 'Severe')
                  AND timestamp >= '{start_local}'
                  AND timestamp <= '{end_local}'
                ORDER BY timestamp DESC
                LIMIT 1048575
                """
                security_events_results = self.conn.execute(security_events_query).fetchall()
                classification_stats["security_events"] = [
                    {
                        "timestamp": ts,
                        "client_ip": ip,
                        "classification": cls,
                        "reason": reason,
                        "url": url,
                        "status_code": status,
                    }
                    for ts, ip, cls, reason, url, status in security_events_results
                ]
            except Exception as e:
                logger.debug(f"Classification í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                classification_stats = {}

            if progress is not None and task_id is not None:
                progress.update(task_id, description="[cyan]ëŠë¦° ì‘ë‹µ/ë°”ì´íŠ¸ ë¶„ì„ ì™„ë£Œ...")
                progress.advance(task_id)

            # 4) ìƒíƒœ ì½”ë“œë³„ ë¡œê·¸ ìˆ˜ì§‘
            if progress is not None and task_id is not None:
                progress.update(task_id, description="[cyan]ELB ìƒíƒœ ì½”ë“œë³„ ë¡œê·¸ ìˆ˜ì§‘ ì¤‘...")
            status_code_logs = {}
            for status_prefix, log_key in [
                ("2", "ELB 2xx Count"),
                ("3", "ELB 3xx Count"),
                ("4", "ELB 4xx Count"),
                ("5", "ELB 5xx Count"),
            ]:
                query = f"""
                SELECT timestamp,
                       client_ip,
                       target_ip,
                       target_port,
                       target,
                       http_method,
                       url,
                       elb_status_code,
                       target_status_code,
                       response_time,
                       received_bytes,
                       sent_bytes,
                       user_agent,
                       redirect_url,
                       error_reason,
                       target_group_arn,
                       target_group_name
                FROM alb_logs
                WHERE elb_status_code LIKE '{status_prefix}%'
                  AND elb_status_code != '-'
                  AND elb_status_code IS NOT NULL
                  AND timestamp IS NOT NULL
                  AND timestamp >= '{start_local}'
                  AND timestamp <= '{end_local}'
                ORDER BY timestamp DESC
                """
                results = self.conn.execute(query).fetchall()
                logs_list = []
                timestamps_list = []

                for row in results:
                    # íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ì´ë¯¸ ì‚¬ìš©ì íƒ€ì„ì¡´ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ìˆìŒ
                    local_timestamp = row[0]

                    log_dict = {
                        "timestamp": local_timestamp,
                        "client_ip": row[1],
                        "target_ip": row[2],
                        "target_port": row[3],
                        "target": row[4],
                        "http_method": row[5],
                        "request": row[6],
                        "elb_status_code": row[7],
                        "target_status_code": row[8],
                        "response_time": row[9],
                        "received_bytes": row[10],
                        "sent_bytes": row[11],
                        "user_agent": row[12],
                        "redirect_url": row[13],
                        "error_reason": row[14],
                        "target_group_arn": row[15],
                        "target_group_name": row[16],
                    }
                    logs_list.append(log_dict)
                    timestamps_list.append(local_timestamp)

                status_code_logs[log_key] = {
                    "full_logs": logs_list,
                    "timestamps": timestamps_list,
                    "count": len(logs_list),
                    "fill": None,
                }

                # íƒ€ì„ìŠ¤íƒ¬í”„ ë²„ì „ë„ ì¶”ê°€
                timestamp_key = log_key.replace("Count", "Timestamp")
                status_code_logs[timestamp_key] = {
                    "full_logs": logs_list,
                    "timestamps": timestamps_list,
                    "count": len(logs_list),
                    "fill": None,
                }

            # Backend ìƒíƒœ ì½”ë“œë³„ ë¡œê·¸
            if progress is not None and task_id is not None:
                progress.update(task_id, description="[cyan]Backend ìƒíƒœ ì½”ë“œë³„ ë¡œê·¸ ìˆ˜ì§‘ ì¤‘...")
            for status_prefix, log_key in [
                ("4", "Backend 4xx Count"),
                ("5", "Backend 5xx Count"),
            ]:
                query = f"""
                SELECT timestamp,
                       client_ip,
                       target_ip,
                       target_port,
                       target,
                       http_method,
                       url,
                       elb_status_code,
                       target_status_code,
                       response_time,
                       received_bytes,
                       sent_bytes,
                       user_agent,
                       error_reason,
                       target_group_arn,
                       target_group_name
                FROM alb_logs
                WHERE target_status_code LIKE '{status_prefix}%'
                  AND target_status_code != '-'
                  AND target_status_code IS NOT NULL
                  AND timestamp IS NOT NULL
                  AND timestamp >= '{start_local}'
                  AND timestamp <= '{end_local}'
                ORDER BY timestamp DESC
                """
                results = self.conn.execute(query).fetchall()
                logs_list = []
                timestamps_list = []

                for row in results:
                    # íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ì´ë¯¸ ì‚¬ìš©ì íƒ€ì„ì¡´ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ìˆìŒ
                    local_timestamp = row[0]

                    log_dict = {
                        "timestamp": local_timestamp,
                        "client_ip": row[1],
                        "target_ip": row[2],
                        "target_port": row[3],
                        "target": row[4],
                        "http_method": row[5],
                        "request": row[6],
                        "elb_status_code": row[7],
                        "target_status_code": row[8],
                        "response_time": row[9],
                        "received_bytes": row[10],
                        "sent_bytes": row[11],
                        "user_agent": row[12],
                        "error_reason": row[13],
                        "target_group_arn": row[14],
                        "target_group_name": row[15],
                    }
                    logs_list.append(log_dict)
                    timestamps_list.append(local_timestamp)

                status_code_logs[log_key] = {
                    "full_logs": logs_list,
                    "timestamps": timestamps_list,
                    "count": len(logs_list),
                    "fill": None,
                }

                # íƒ€ì„ìŠ¤íƒ¬í”„ ë²„ì „ë„ ì¶”ê°€
                timestamp_key = log_key.replace("Count", "Timestamp")
                status_code_logs[timestamp_key] = {
                    "full_logs": logs_list,
                    "timestamps": timestamps_list,
                    "count": len(logs_list),
                    "fill": None,
                }

            # ìƒíƒœ ì½”ë“œ ìˆ˜ì§‘ ë‹¨ê³„ ì™„ë£Œ ë°˜ì˜ (ELB + Backend)
            if progress is not None and task_id is not None:
                progress.advance(task_id)
                progress.advance(task_id)

            if progress is not None and task_id is not None:
                progress.update(task_id, description="[cyan]êµ­ê°€ ì •ë³´ ë§¤í•‘ ì¤‘...")

            # ì‹œì‘/ì¢…ë£Œ ì‹œê°„ í¬ë§·íŒ… - ì‚¬ìš©ìê°€ ì„¤ì •í•œ ë¶„ì„ ê¸°ê°„ ì‚¬ìš©
            start_time = self.start_datetime.strftime("%Y-%m-%d %H:%M:%S")
            end_time = self.end_datetime.strftime("%Y-%m-%d %H:%M:%S")

            # ì‹¤ì œ ë¡œê·¸ ë°ì´í„°ì˜ ì‹œê°„ ë²”ìœ„ - ì´ë¯¸ ì‚¬ìš©ì íƒ€ì„ì¡´ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ìˆìŒ
            actual_start_time = summary_result[2].strftime("%Y-%m-%d %H:%M:%S") if summary_result[2] else "N/A"

            actual_end_time = summary_result[3].strftime("%Y-%m-%d %H:%M:%S") if summary_result[3] else "N/A"

            # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
            analysis_results = {
                # ê¸°ë³¸ ì •ë³´
                "start_time": start_time,
                "end_time": end_time,
                "actual_start_time": actual_start_time,
                "actual_end_time": actual_end_time,
                "timezone": self.timezone.zone,
                "log_lines_count": summary_result[0],
                "log_files_count": getattr(self, "loaded_log_files_count", 0),
                "log_files_path": getattr(self, "loaded_log_directory", ""),
                "unique_client_ips": summary_result[1],
                "total_received_bytes": summary_result[5] or 0,
                "total_sent_bytes": summary_result[6] or 0,
                # S3 ì •ë³´
                "s3_bucket_name": self.bucket_name,
                "s3_prefix": self.prefix,
                "s3_uri": f"s3://{self.bucket_name}/{self.prefix}",
                # ì¹´ìš´íŠ¸ ë°ì´í„°
                "elb_2xx_count": summary_result[7] or 0,
                "elb_3xx_count": summary_result[8] or 0,
                "elb_4xx_count": summary_result[9] or 0,
                "elb_5xx_count": summary_result[10] or 0,
                "backend_4xx_count": summary_result[11] or 0,
                "backend_5xx_count": summary_result[12] or 0,
                "long_response_count": long_response_count_val,
                # ì¹´ìš´íŠ¸ ë°ì´í„°
                "client_ip_counts": client_ip_counts,
                "request_url_counts": request_url_counts,
                "user_agent_counts": user_agent_counts,
                "client_status_statistics": client_status_statistics,
                "target_status_statistics": target_status_statistics,
                "request_url_details": request_url_details,
                "long_response_times": long_response_times,
                "received_bytes": received_bytes,
                "sent_bytes": sent_bytes,
                # ì¶”ê°€ ë¶„ì„ ë°ì´í„°
                "response_time_percentiles": response_time_percentiles,
                "error_reason_counts": error_reason_counts,
                "target_request_stats": target_request_stats,
                "url_error_stats": url_error_stats,
                # ì„±ëŠ¥ ë¶„ì„ ë°ì´í„° (TPS, SLA, Target Latency)
                "tps_time_series": tps_time_series,
                "sla_compliance": sla_compliance,
                "target_latency_stats": target_latency_stats,
                "response_time_distribution": response_time_distribution,
                "bucket_minutes": bucket_minutes,
                # ì¶”ê°€ ë¶„ì„ ë°ì´í„° (Phase 2)
                "processing_time_breakdown": processing_time_breakdown,
                "connection_failures": connection_failures,
                "http_version_distribution": http_version_distribution,
                "ssl_stats": ssl_stats,
                "actions_stats": actions_stats,
                "classification_stats": classification_stats,
                # ë¹ˆ ë°ì´í„° (í˜¸í™˜ì„±)
                "elb_error_timestamps": [],
                "backend_error_timestamps": [],
                "elb_2xx_timestamps": [],
                "elb_3xx_timestamps": [],
                "elb_4xx_timestamps": [],
                "elb_5xx_timestamps": [],
                "backend_4xx_timestamps": [],
                "backend_5xx_timestamps": [],
            }

            # elb/alb ì´ë¦„ ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
            try:
                alb_name_row = self.conn.execute(
                    "SELECT elb_name FROM alb_logs WHERE elb_name IS NOT NULL AND elb_name != '' LIMIT 1"
                ).fetchone()
                if alb_name_row and alb_name_row[0]:
                    analysis_results["alb_name"] = alb_name_row[0]
            except Exception:
                pass  # nosec B110 - ALB name extraction is optional

            # ìƒíƒœ ì½”ë“œë³„ ë¡œê·¸ ë°ì´í„° ì¶”ê°€
            analysis_results.update(status_code_logs)

            # ğŸŒ êµ­ê°€ ì •ë³´ ì¶”ê°€ (IPIntelligence í†µí•© API ì‚¬ìš©)
            try:
                if self.ip_intel.initialize():
                    logger.debug("ğŸŒ IP êµ­ê°€ ì •ë³´ ë§¤í•‘ ì‹œì‘...")

                    # ê³ ìœ í•œ í´ë¼ì´ì–¸íŠ¸ IP ëª©ë¡ ì¶”ì¶œ
                    unique_ips = list(client_ip_counts.keys())

                    # ìƒìœ„ 10ê°œ IP ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
                    top_ips = sorted(client_ip_counts.items(), key=lambda x: x[1], reverse=True)[:10]
                    logger.debug(f"ğŸ” ìƒìœ„ 10ê°œ í´ë¼ì´ì–¸íŠ¸ IP: {[ip for ip, count in top_ips]}")

                    # êµ­ê°€ ì •ë³´ ë§¤í•‘
                    country_mapping = self.ip_intel.get_country_codes_batch(unique_ips)

                    # êµ­ê°€ë³„ í†µê³„ ìƒì„±
                    country_stats = self.ip_intel.get_country_statistics(unique_ips)

                    # ê²°ê³¼ì— ì¶”ê°€
                    analysis_results["ip_country_mapping"] = country_mapping
                    analysis_results["country_statistics"] = country_stats

                    # ìƒìœ„ 10ê°œ IPì˜ êµ­ê°€ ë§¤í•‘ ê²°ê³¼ ì¶œë ¥
                    top_ip_countries = [(ip, country_mapping.get(ip, "UNKNOWN")) for ip, count in top_ips]
                    logger.debug(f"ğŸŒ ìƒìœ„ 10ê°œ IP êµ­ê°€ ë§¤í•‘: {top_ip_countries}")

                    logger.debug(f"âœ… êµ­ê°€ ì •ë³´ ë§¤í•‘ ì™„ë£Œ: {len(country_mapping)}ê°œ IP, {len(country_stats)}ê°œ êµ­ê°€")
                else:
                    logger.warning("âš ï¸ IP-Country ë§¤í¼ ì´ˆê¸°í™” ì‹¤íŒ¨, êµ­ê°€ ì •ë³´ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                    analysis_results["ip_country_mapping"] = {}
                    analysis_results["country_statistics"] = {}
            except Exception as e:
                logger.error(f"âŒ êµ­ê°€ ì •ë³´ ë§¤í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                analysis_results["ip_country_mapping"] = {}
                analysis_results["country_statistics"] = {}
            finally:
                # êµ­ê°€ ì •ë³´ ë§¤í•‘ ë‹¨ê³„ ì™„ë£Œ ë°˜ì˜
                if progress is not None and task_id is not None:
                    progress.advance(task_id)

            return analysis_results

        except Exception as e:
            logger.error(f"âŒ DuckDB ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return self._get_empty_analysis_results()

    def _get_empty_analysis_results(self) -> dict[str, Any]:
        """ë¹ˆ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            # ê¸°ë³¸ ì •ë³´
            "start_time": self.start_datetime.strftime("%Y-%m-%d %H:%M:%S"),
            "end_time": self.end_datetime.strftime("%Y-%m-%d %H:%M:%S"),
            "actual_start_time": "N/A",
            "actual_end_time": "N/A",
            "timezone": self.timezone.zone,
            "log_lines_count": 0,
            "log_files_count": 0,
            "log_files_path": "",
            "unique_client_ips": 0,
            "total_received_bytes": 0,
            "total_sent_bytes": 0,
            # S3 ì •ë³´
            "s3_bucket_name": self.bucket_name,
            "s3_prefix": self.prefix,
            "s3_uri": f"s3://{self.bucket_name}/{self.prefix}",
            # ì¹´ìš´íŠ¸ ë°ì´í„°
            "elb_2xx_count": 0,
            "elb_3xx_count": 0,
            "elb_4xx_count": 0,
            "elb_5xx_count": 0,
            "backend_4xx_count": 0,
            "backend_5xx_count": 0,
            "long_response_count": 0,
            # íƒ€ì„ìŠ¤íƒ¬í”„
            "elb_error_timestamps": [],
            "backend_error_timestamps": [],
            "elb_2xx_timestamps": [],
            "elb_3xx_timestamps": [],
            "elb_4xx_timestamps": [],
            "elb_5xx_timestamps": [],
            "backend_4xx_timestamps": [],
            "backend_5xx_timestamps": [],
            # ì¹´ìš´íŠ¸ ë°ì´í„°
            "client_ip_counts": {},
            "client_status_statistics": {},
            "target_status_statistics": {},
            "request_url_counts": {},
            "user_agent_counts": {},
            "abuse_ips": [],
            "abuse_ips_list": [],
            "abuse_ip_details": {},
            "long_response_times": [],
            "received_bytes": {},
            "sent_bytes": {},
            # ì¶”ê°€ ë¶„ì„ ë°ì´í„°
            "response_time_percentiles": {},
            "error_reason_counts": {},
            "target_request_stats": {},
            "url_error_stats": {},
            # ì„±ëŠ¥ ë¶„ì„ ë°ì´í„° (TPS, SLA, Target Latency)
            "tps_time_series": [],
            "sla_compliance": {},
            "target_latency_stats": {},
            "response_time_distribution": {},
            "bucket_minutes": 15,
            # ì¶”ê°€ ë¶„ì„ ë°ì´í„° (Phase 2)
            "processing_time_breakdown": {},
            "connection_failures": {},
            "http_version_distribution": {},
            "ssl_stats": {},
            "actions_stats": {},
            "classification_stats": {},
            # êµ­ê°€ ì •ë³´
            "ip_country_mapping": {},
            "country_statistics": {},
            # ì „ì²´ ë¡œê·¸ ë°ì´í„°
            "ELB 2xx Count": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "ELB 3xx Count": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "ELB 4xx Count": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "ELB 5xx Count": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "Backend 4xx Count": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "Backend 5xx Count": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "ELB 2xx Timestamp": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "ELB 3xx Timestamp": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "ELB 4xx Timestamp": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "ELB 5xx Timestamp": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "Backend 4xx Timestamp": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "Backend 5xx Timestamp": {
                "full_logs": [],
                "timestamps": [],
                "count": 0,
                "fill": None,
            },
            "request_url_details": {},
        }

    def clean_up(self, directories: list[str]) -> None:
        """ì„ì‹œ íŒŒì¼ ë° ë””ë ‰í† ë¦¬ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        try:
            # DuckDB ì—°ê²° ì •ë¦¬
            if hasattr(self, "conn") and self.conn:
                self.conn.close()
                logger.debug("âœ… DuckDB ì—°ê²° ì •ë¦¬ ì™„ë£Œ")

            # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ëª…ì‹œì  ì •ë¦¬
            if hasattr(self, "download_dir") and os.path.exists(self.download_dir):
                try:
                    logger.debug(f"ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì¤‘: {self.download_dir}")
                    shutil.rmtree(self.download_dir, ignore_errors=True)
                    logger.debug(f"âœ… ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ: {self.download_dir}")
                except Exception as e:
                    logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {self.download_dir}, ì˜¤ë¥˜: {str(e)}")

            # ì••ì¶• í•´ì œ ë””ë ‰í† ë¦¬ ëª…ì‹œì  ì •ë¦¬
            if hasattr(self, "decompressed_dir") and os.path.exists(self.decompressed_dir):
                try:
                    logger.debug(f"ì••ì¶• í•´ì œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì¤‘: {self.decompressed_dir}")
                    shutil.rmtree(self.decompressed_dir, ignore_errors=True)
                    logger.debug(f"âœ… ì••ì¶• í•´ì œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ: {self.decompressed_dir}")
                except Exception as e:
                    logger.error(f"âŒ ì••ì¶• í•´ì œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {self.decompressed_dir}, ì˜¤ë¥˜: {str(e)}")

            # DuckDB ì‘ì—… ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if (
                hasattr(self, "temp_work_dir")
                and isinstance(self.temp_work_dir, str)
                and os.path.exists(self.temp_work_dir)
            ):
                try:
                    logger.debug(f"ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì¤‘: {self.temp_work_dir}")
                    shutil.rmtree(self.temp_work_dir, ignore_errors=True)
                    logger.debug(f"âœ… ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ: {self.temp_work_dir}")
                except Exception as e:
                    logger.error(f"âŒ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {self.temp_work_dir}, ì˜¤ë¥˜: {str(e)}")

            # DuckDB íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ì •ë¦¬ (ì¼íšŒì„± ë¶„ì„ì´ë¯€ë¡œ ì‚­ì œ)
            if (
                hasattr(self, "duckdb_db_path")
                and isinstance(self.duckdb_db_path, str)
                and os.path.exists(self.duckdb_db_path)
            ):
                try:
                    logger.debug(f"DuckDB íŒŒì¼ ì‚­ì œ ì¤‘: {self.duckdb_db_path}")
                    os.remove(self.duckdb_db_path)
                    logger.debug(f"âœ… DuckDB íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {self.duckdb_db_path}")
                except Exception as e:
                    logger.error(f"âŒ DuckDB íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {self.duckdb_db_path}, ì˜¤ë¥˜: {str(e)}")

            if hasattr(self, "duckdb_dir") and isinstance(self.duckdb_dir, str) and os.path.isdir(self.duckdb_dir):
                try:
                    # ë¹„ì–´ ìˆìœ¼ë©´ ì œê±°
                    if not os.listdir(self.duckdb_dir):
                        os.rmdir(self.duckdb_dir)
                except Exception:
                    pass  # nosec B110 - Directory cleanup is best-effort

            # ê¸°ì¡´ì— ì „ë‹¬ëœ ë””ë ‰í† ë¦¬ë„ ì •ë¦¬
            already_cleaned = []
            if hasattr(self, "download_dir"):
                already_cleaned.append(self.download_dir)
            if hasattr(self, "decompressed_dir"):
                already_cleaned.append(self.decompressed_dir)

            for directory in directories:
                # ì´ë¯¸ ì²˜ë¦¬í•œ ë””ë ‰í† ë¦¬ë©´ ìŠ¤í‚µ
                if directory in already_cleaned:
                    logger.debug(f"ìŠ¤í‚µ: ì´ë¯¸ ì •ë¦¬ëœ ë””ë ‰í† ë¦¬ - {directory}")
                    continue

                if not isinstance(directory, str):
                    logger.warning(f"ìŠ¤í‚µ: ë””ë ‰í† ë¦¬ê°€ ë¬¸ìì—´ì´ ì•„ë‹˜ - {type(directory)}: {directory}")
                    continue

                if os.path.exists(directory):
                    try:
                        logger.debug(f"ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì¤‘: {directory}")
                        shutil.rmtree(directory, ignore_errors=True)
                        logger.debug(f"âœ… ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ: {directory}")
                    except Exception as e:
                        logger.error(f"âŒ ë””ë ‰í† ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {directory}, ì˜¤ë¥˜: {str(e)}")
        except Exception as e:
            logger.error(f"ì •ë¦¬ ê³¼ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸš€ DuckDB ê¸°ë°˜ ALB ë¡œê·¸ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")

    # ìƒ˜í”Œ ë¡œê·¸ ë””ë ‰í† ë¦¬ë¡œ í…ŒìŠ¤íŠ¸
    log_dir = "data/log"
    if os.path.exists(log_dir):
        # ë”ë¯¸ ë§¤ê°œë³€ìˆ˜ë¡œ ë¶„ì„ê¸° ìƒì„±
        analyzer = ALBLogAnalyzer(
            s3_client=None,
            bucket_name="test",
            prefix="test",
            start_datetime=datetime.now(),
        )

        results = analyzer.analyze_logs(log_dir)
        print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼: {len(results)}ê°œ ì¹´í…Œê³ ë¦¬")

        for key, value in results.items():
            if isinstance(value, list):
                print(f"  - {key}: {len(value)}ê°œ í•­ëª©")
            elif isinstance(value, dict):
                print(f"  - {key}: {len(value)}ê°œ í•„ë“œ")
            else:
                print(f"  - {key}: {value}")

        analyzer.clean_up([])
    else:
        print(f"âŒ ë¡œê·¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_dir}")
