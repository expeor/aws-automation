"""
core/tools/analysis/log/alb_analyzer.py - ALB ë¡œê·¸ ë¶„ì„ ë„êµ¬ ì§„ì…ì 

í”ŒëŸ¬ê·¸ì¸ ê·œì•½:
    - run(ctx): í•„ìˆ˜. ì‹¤í–‰ í•¨ìˆ˜.
    - collect_options(ctx): ì„ íƒ. ì¶”ê°€ ì˜µì…˜ ìˆ˜ì§‘.
"""

from __future__ import annotations

import logging
import os
from datetime import datetime, timedelta
from typing import TYPE_CHECKING, Any

import pytz  # type: ignore[import-untyped]
import questionary
from rich.panel import Panel
from rich.table import Table

from cli.ui import (
    console,
    print_step_header,
    print_sub_info,
    print_sub_task_done,
    print_sub_warning,
)
from core.auth import get_context_session
from core.parallel import get_client
from core.tools.cache import get_cache_dir
from core.tools.output import open_in_explorer

if TYPE_CHECKING:
    from cli.flow.context import ExecutionContext

logger = logging.getLogger(__name__)

# í•„ìš”í•œ AWS ê¶Œí•œ ëª©ë¡
REQUIRED_PERMISSIONS = {
    "read": [
        "elasticloadbalancing:DescribeLoadBalancers",
        "elasticloadbalancing:DescribeLoadBalancerAttributes",
        "s3:ListBucket",
        "s3:GetObject",
        "sts:GetCallerIdentity",
    ],
}


def collect_options(ctx) -> None:
    """ALB ë¡œê·¸ ë¶„ì„ì— í•„ìš”í•œ ì˜µì…˜ ìˆ˜ì§‘

    - S3 ë²„í‚· ê²½ë¡œ (ìë™ íƒìƒ‰ ë˜ëŠ” ìˆ˜ë™ ì…ë ¥)
    - ì‹œê°„ ë²”ìœ„
    - íƒ€ì„ì¡´

    Args:
        ctx: ExecutionContext
    """
    console.print("\n[bold cyan]ALB ë¡œê·¸ ë¶„ì„ ì„¤ì •[/bold cyan]")

    # ì„¸ì…˜ íšë“ (ì²« ë²ˆì§¸ ë¦¬ì „ ì‚¬ìš©)
    region = ctx.regions[0] if ctx.regions else "ap-northeast-2"
    session = get_context_session(ctx, region)

    # 1. S3 ë²„í‚· ê²½ë¡œ ì…ë ¥ ë°©ì‹ ì„ íƒ
    bucket_path = _get_bucket_input_with_options(session, ctx)
    ctx.options["bucket"] = bucket_path

    # 2. ì‹œê°„ ë²”ìœ„ ì…ë ¥
    start_time, end_time = _get_time_range_input()
    ctx.options["start_time"] = start_time
    ctx.options["end_time"] = end_time

    # 3. íƒ€ì„ì¡´ ì…ë ¥
    timezone = _get_timezone_input()
    ctx.options["timezone"] = timezone


def run(ctx: ExecutionContext) -> None:
    """ALB ë¡œê·¸ ë¶„ì„ ì‹¤í–‰

    Args:
        ctx: ExecutionContext (optionsì— bucket, start_time, end_time, timezone í¬í•¨)
    """
    from reports.log_analyzer import ALBLogAnalyzer

    console.print("[bold]ALB ë¡œê·¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...[/bold]")

    # ì˜µì…˜ ì¶”ì¶œ
    bucket = ctx.options.get("bucket")
    start_time = ctx.options.get("start_time")
    end_time = ctx.options.get("end_time")
    timezone = ctx.options.get("timezone", "Asia/Seoul")

    if not bucket:
        console.print("[red]âŒ S3 ë²„í‚· ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/red]")
        return

    # ì„¸ì…˜ íšë“
    region = ctx.regions[0] if ctx.regions else "ap-northeast-2"
    session = get_context_session(ctx, region)
    s3_client = get_client(session, "s3")

    # S3 URI íŒŒì‹±
    if not bucket.startswith("s3://"):
        bucket = f"s3://{bucket}"

    bucket_parts = bucket.split("/")
    bucket_name = bucket_parts[2]
    prefix = "/".join(bucket_parts[3:]) if len(bucket_parts) > 3 else ""

    # ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì • (temp/alb í•˜ìœ„ ì‚¬ìš©)
    alb_cache_dir = get_cache_dir("alb")
    gz_dir = os.path.join(alb_cache_dir, "gz")
    log_dir = os.path.join(alb_cache_dir, "log")
    os.makedirs(gz_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)

    try:
        # Step 1: ë¡œê·¸ ë¶„ì„ê¸° ì´ˆê¸°í™”
        print_step_header(1, "ë¡œê·¸ ë¶„ì„ê¸° ì¤€ë¹„ ì¤‘...")
        analyzer = ALBLogAnalyzer(
            s3_client=s3_client,
            bucket_name=bucket_name,
            prefix=prefix,
            start_datetime=start_time,
            end_datetime=end_time,
            timezone=timezone,
            max_workers=5,
        )

        # Step 2: ë¡œê·¸ ë‹¤ìš´ë¡œë“œ
        print_step_header(2, "ë¡œê·¸ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ ì¤‘...")
        downloaded_files = analyzer.download_logs()
        if not downloaded_files:
            print_sub_warning("ìš”ì²­ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ALB ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print_sub_info("ALBëŠ” 5ë¶„ ë‹¨ìœ„ë¡œ íŒŒì¼ì„ ìƒì„±í•˜ë©°, íŠ¸ë˜í”½ì´ ì—†ìœ¼ë©´ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return

        # ì••ì¶• í•´ì œ
        if isinstance(downloaded_files, list) and downloaded_files:
            gz_directory = os.path.dirname(downloaded_files[0]) if isinstance(downloaded_files[0], str) else gz_dir
        else:
            gz_directory = gz_dir

        log_directory = analyzer.decompress_logs(gz_directory)

        # Step 3: ë¡œê·¸ ë¶„ì„
        print_step_header(3, "ë¡œê·¸ ë¶„ì„ ì¤‘...")
        analysis_results = analyzer.analyze_logs(log_directory)

        # abuse_ips ì²˜ë¦¬
        if isinstance(analysis_results.get("abuse_ips"), (dict, set)):
            analysis_results["abuse_ips_list"] = list(analysis_results.get("abuse_ips", set()))
            analysis_results["abuse_ips"] = "AbuseIPDB IPs processed"

        # Step 4: ë³´ê³ ì„œ ìƒì„±
        total_logs = analysis_results.get("log_lines_count", 0)
        print_sub_task_done(f"ë°ì´í„° í¬ê¸°: {total_logs:,}ê°œ ë¡œê·¸ ë¼ì¸")
        print_step_header(4, "ë³´ê³ ì„œ ìƒì„± ì¤‘...")

        # ì¶œë ¥ ê²½ë¡œ ìƒì„±
        output_dir = _create_output_directory(ctx)

        # ë¦¬í¬íŠ¸ ìƒì„± (ctx.output_configì— ë”°ë¼ Excel, HTML, ë˜ëŠ” ë‘˜ ë‹¤)
        report_paths = _generate_reports(ctx, analyzer, analysis_results, output_dir)

        from core.tools.output import print_report_complete

        print_report_complete(report_paths)

        # Step 5: ì„ì‹œ íŒŒì¼ ì •ë¦¬
        _cleanup_temp_files(analyzer, gz_directory, log_directory)

        # ìë™ìœ¼ë¡œ ë³´ê³ ì„œ í´ë” ì—´ê¸° (Excelì´ ìˆìœ¼ë©´ Explorerë¡œ, HTMLë§Œ ìˆìœ¼ë©´ ë¸Œë¼ìš°ì €ë¡œ)
        if report_paths.get("excel"):
            open_in_explorer(os.path.dirname(report_paths["excel"]))

    except Exception as e:
        console.print(f"[red]âŒ ALB ë¡œê·¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}[/red]")
        raise


# =============================================================================
# í—¬í¼ í•¨ìˆ˜ë“¤
# =============================================================================


def _generate_reports(ctx, analyzer, analysis_results: dict[str, Any], output_dir: str) -> dict[str, str]:
    """ì¶œë ¥ ì„¤ì •ì— ë”°ë¼ Excel/HTML ë³´ê³ ì„œ ìƒì„±

    Args:
        ctx: ExecutionContext (output_config í¬í•¨)
        analyzer: ALBLogAnalyzer ì¸ìŠ¤í„´ìŠ¤
        analysis_results: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬

    Returns:
        ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬ {"excel": "...", "html": "..."}
    """
    from reports.log_analyzer import ALBExcelReporter

    report_paths: dict[str, str] = {}

    # ì¶œë ¥ ì„¤ì • í™•ì¸
    should_excel = ctx.should_output_excel() if hasattr(ctx, "should_output_excel") else True
    should_html = ctx.should_output_html() if hasattr(ctx, "should_output_html") else True

    # Excel ë³´ê³ ì„œ ìƒì„± (ê¸°ë³¸ - ìµœì í™”ëœ ìƒì„¸ ë¦¬í¬íŠ¸)
    if should_excel:
        report_filename = _generate_report_filename(analyzer, analysis_results)
        report_path = os.path.join(output_dir, report_filename)

        reporter = ALBExcelReporter(data=analysis_results, output_dir=output_dir)
        final_report_path = reporter.generate_report(report_path)
        report_paths["excel"] = final_report_path

    # HTML ë³´ê³ ì„œ ìƒì„± (ìš”ì•½ ëŒ€ì‹œë³´ë“œ)
    if should_html:
        console.print("   HTML ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        html_path = _generate_html_report(ctx, analyzer, analysis_results, output_dir)
        if html_path:
            console.print("   [green]âœ“ HTML ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ[/green]")
            report_paths["html"] = html_path

            # HTMLë§Œ ìƒì„±í•œ ê²½ìš° ë¸Œë¼ìš°ì €ì—ì„œ ìë™ ì—´ê¸°
            if not should_excel:
                from core.tools.io.html import open_in_browser

                output_config = ctx.get_output_config() if hasattr(ctx, "get_output_config") else None
                if output_config is None or output_config.auto_open:
                    open_in_browser(html_path)

    return report_paths


def _generate_html_report(ctx, analyzer, analysis_results: dict[str, Any], output_dir: str) -> str | None:
    """HTML ìš”ì•½ ë³´ê³ ì„œ ìƒì„±

    ALB ë¡œê·¸ ë¶„ì„ì˜ ì£¼ìš” ì§€í‘œë¥¼ ì‹œê°í™”í•˜ëŠ” HTML ëŒ€ì‹œë³´ë“œ ìƒì„±.
    Excel ë¦¬í¬íŠ¸ì˜ ìƒì„¸ ë°ì´í„°ë¥¼ ë³´ì™„í•˜ëŠ” ìš©ë„.

    ë ˆì´ì•„ì›ƒ:
        [ê°œìš” ì„¹ì…˜] ELB ìƒíƒœ ì½”ë“œ / ì •ìƒ ì‘ë‹µ ë¹„ìœ¨ ê²Œì´ì§€ / ì‹œê°„ëŒ€ë³„ ìš”ì²­ íŠ¸ë Œë“œ
        [íŠ¸ë˜í”½ ë¶„ì„ ì„¹ì…˜] êµ­ê°€ë³„ ë¶„í¬ / Top í´ë¼ì´ì–¸íŠ¸ IP / Top ìš”ì²­ URL
        [ìš”ì²­ ë¶„ì„ ì„¹ì…˜] HTTP ë©”ì„œë“œ / User-Agent / í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ ì½”ë“œ / Targetë³„ ë¶„í¬
        [ì„±ëŠ¥ ë¶„ì„ ì„¹ì…˜] ì‘ë‹µ ì‹œê°„ ë°±ë¶„ìœ„ / ì‘ë‹µ ì‹œê°„ êµ¬ê°„ ë¶„í¬ / ë°ì´í„° ì „ì†¡ëŸ‰ / Backend ìƒíƒœ ì½”ë“œ
        [ì—ëŸ¬ ë¶„ì„ ì„¹ì…˜] ELB vs Backend ì—ëŸ¬ / ì—ëŸ¬ ì›ì¸ / URLë³„ ì—ëŸ¬ìœ¨ / ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ìœ¨ / ì—ëŸ¬ íŠ¸ë Œë“œ
    """
    try:
        from core.tools.io.html import HTMLReport

        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        total_logs = analysis_results.get("log_lines_count", 0)
        alb_name = analysis_results.get("alb_name", "ALB")
        unique_ips = analysis_results.get("unique_client_ips", 0)

        # ì—ëŸ¬ ì¹´ìš´íŠ¸
        elb_2xx = analysis_results.get("elb_2xx_count", 0)
        elb_3xx = analysis_results.get("elb_3xx_count", 0)
        elb_4xx = analysis_results.get("elb_4xx_count", 0)
        elb_5xx = analysis_results.get("elb_5xx_count", 0)
        backend_4xx = analysis_results.get("backend_4xx_count", 0)
        backend_5xx = analysis_results.get("backend_5xx_count", 0)
        long_response = analysis_results.get("long_response_count", 0)

        # ì •ìƒ ì‘ë‹µ ë¹„ìœ¨ ê³„ì‚°
        total_responses = elb_2xx + elb_3xx + elb_4xx + elb_5xx
        success_rate = ((elb_2xx + elb_3xx) / total_responses * 100) if total_responses > 0 else 0

        # ì‹¤ì œ ìš”ì²­í•œ IP ì¤‘ ì•…ì„± IP ë§¤ì¹­
        client_ip_counts = analysis_results.get("client_ip_counts", {})
        abuse_ips_all = set(analysis_results.get("abuse_ips_list", []))
        matching_abuse_ips = [(ip, client_ip_counts.get(ip, 0)) for ip in client_ip_counts if ip in abuse_ips_all]
        matching_abuse_ips = sorted(matching_abuse_ips, key=lambda x: -x[1])
        abuse_count = len(matching_abuse_ips)

        # HTMLReport ìƒì„±
        subtitle = f"ë¶„ì„ ê¸°ê°„: {analyzer.start_datetime.strftime('%Y-%m-%d %H:%M')} ~ {analyzer.end_datetime.strftime('%Y-%m-%d %H:%M')}"
        report = HTMLReport(title=f"ALB ë¡œê·¸ ë¶„ì„: {alb_name}", subtitle=subtitle)

        # ìš”ì•½ ì¹´ë“œ
        report.add_summary(
            [
                ("ì´ ìš”ì²­", f"{total_logs:,}", None),
                ("ê³ ìœ  IP", f"{unique_ips:,}", None),
                ("ì •ìƒ ì‘ë‹µë¥ ", f"{success_rate:.1f}%", "success" if success_rate >= 99 else None),
                ("ELB 4xx", f"{elb_4xx:,}", "warning" if elb_4xx > 0 else None),
                ("ELB 5xx", f"{elb_5xx:,}", "danger" if elb_5xx > 0 else None),
                ("Backend 5xx", f"{backend_5xx:,}", "danger" if backend_5xx > 0 else None),
                ("ëŠë¦° ì‘ë‹µ (â‰¥1s)", f"{long_response:,}", "warning" if long_response > 0 else None),
                ("ì•…ì„± IP", f"{abuse_count:,}", "danger" if abuse_count > 0 else None),
            ]
        )

        # =============================================================================
        # [ê°œìš” ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸ“Š ê°œìš”")

        # 1. ELB ìƒíƒœ ì½”ë“œ ë¶„í¬ (ë„ë„› ì°¨íŠ¸)
        status_data = [
            ("2xx ì„±ê³µ", elb_2xx),
            ("3xx ë¦¬ë‹¤ì´ë ‰íŠ¸", elb_3xx),
            ("4xx í´ë¼ì´ì–¸íŠ¸ ì—ëŸ¬", elb_4xx),
            ("5xx ì„œë²„ ì—ëŸ¬", elb_5xx),
        ]
        status_data = [(name, count) for name, count in status_data if count > 0]
        if status_data:
            report.add_pie_chart("ELB ìƒíƒœ ì½”ë“œ ë¶„í¬", status_data, doughnut=True)

        # 2. ì •ìƒ ì‘ë‹µ ë¹„ìœ¨ ê²Œì´ì§€ (NEW)
        report.add_gauge_chart(
            "ì •ìƒ ì‘ë‹µ ë¹„ìœ¨",
            value=success_rate,
            max_value=100,
            thresholds=[(0.9, "#ee6666"), (0.95, "#fac858"), (1, "#91cc75")],
            unit="%",
        )

        # 3. ì‹œê°„ëŒ€ë³„ ìš”ì²­ íŠ¸ë Œë“œ (ë¼ì¸ ì°¨íŠ¸) - CloudWatch ìŠ¤íƒ€ì¼ ì ì‘í˜• í•´ìƒë„
        all_timestamps: list[datetime] = []
        is_error_list: list[int | float] = []

        for key in ["ELB 2xx Count", "ELB 3xx Count", "ELB 4xx Count", "ELB 5xx Count"]:
            log_data = analysis_results.get(key, {})
            if isinstance(log_data, dict):
                timestamps = log_data.get("timestamps", [])
                is_error = 1 if ("4xx" in key or "5xx" in key) else 0
                for ts in timestamps:
                    if ts and hasattr(ts, "timestamp"):
                        all_timestamps.append(ts)
                        is_error_list.append(is_error)

        if all_timestamps:
            values: dict[str, list[int | float]] = {
                "ì „ì²´ ìš”ì²­": [1] * len(all_timestamps),
                "ì—ëŸ¬ (4xx+5xx)": is_error_list,
            }
            report.add_time_series_chart(
                "ì‹œê°„ëŒ€ë³„ ìš”ì²­ íŠ¸ë Œë“œ",
                timestamps=all_timestamps,
                values=values,
                aggregation="sum",
                area=True,
            )

        # =============================================================================
        # [íŠ¸ë˜í”½ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸŒ íŠ¸ë˜í”½ ë¶„ì„")

        # 4. êµ­ê°€ë³„ ìš”ì²­ ë¶„í¬ (ë°” ì°¨íŠ¸)
        country_stats = analysis_results.get("country_statistics", {})
        if country_stats:
            sorted_countries = [
                (country, data.get("count", 0) if isinstance(data, dict) else data)
                for country, data in country_stats.items()
            ]
            sorted_countries = sorted(sorted_countries, key=lambda x: -x[1])[:15]
            if sorted_countries:
                countries = [c[0] for c in sorted_countries]
                counts = [c[1] for c in sorted_countries]
                report.add_bar_chart(
                    "êµ­ê°€ë³„ ìš”ì²­ ë¶„í¬",
                    categories=countries,
                    series=[("ìš”ì²­ ìˆ˜", counts)],
                    horizontal=True,
                )

        # 5. Top í´ë¼ì´ì–¸íŠ¸ IP (ë°” ì°¨íŠ¸)
        if client_ip_counts:
            sorted_ips = sorted(client_ip_counts.items(), key=lambda x: -x[1])[:10]
            if sorted_ips:
                ips = [ip for ip, _ in sorted_ips]
                counts = [count for _, count in sorted_ips]
                report.add_bar_chart(
                    "Top í´ë¼ì´ì–¸íŠ¸ IP",
                    categories=ips,
                    series=[("ìš”ì²­ ìˆ˜", counts)],
                    horizontal=True,
                )

        # 6. Top ìš”ì²­ URL (ë°” ì°¨íŠ¸)
        url_counts = analysis_results.get("request_url_counts", {})
        if url_counts:
            sorted_urls = sorted(url_counts.items(), key=lambda x: -x[1])[:15]
            if sorted_urls:
                urls = [url[:60] + "..." if len(url) > 60 else url for url, _ in sorted_urls]
                url_counts_list = [count for _, count in sorted_urls]
                report.add_bar_chart(
                    "Top ìš”ì²­ URL",
                    categories=urls,
                    series=[("ìš”ì²­ ìˆ˜", url_counts_list)],
                    horizontal=True,
                )

        # =============================================================================
        # [ìš”ì²­ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸ“ ìš”ì²­ ë¶„ì„")

        # 7. HTTP ë©”ì„œë“œ ë¶„í¬ (ë„ë„› ì°¨íŠ¸)
        request_url_details = analysis_results.get("request_url_details", {})
        if request_url_details:
            method_totals: dict[str, int] = {}
            for url_detail in request_url_details.values():
                if isinstance(url_detail, dict):
                    methods = url_detail.get("methods", {})
                    for method, cnt in methods.items():
                        if method and isinstance(cnt, int):
                            method_totals[method] = method_totals.get(method, 0) + cnt
            if method_totals:
                sorted_methods = sorted(method_totals.items(), key=lambda x: -x[1])
                method_data: list[tuple[str, int | float]] = [(m, c) for m, c in sorted_methods if m.strip()]
                if method_data:
                    report.add_pie_chart("HTTP ë©”ì„œë“œ ë¶„í¬", method_data, doughnut=True)

        # 8. Top User-Agent (ë°” ì°¨íŠ¸)
        user_agent_counts = analysis_results.get("user_agent_counts", {})
        if user_agent_counts:
            sorted_uas = sorted(user_agent_counts.items(), key=lambda x: -x[1])[:10]
            if sorted_uas:
                uas = [ua[:50] + "..." if len(ua) > 50 else ua for ua, _ in sorted_uas]
                counts = [count for _, count in sorted_uas]
                report.add_bar_chart(
                    "Top User-Agent",
                    categories=uas,
                    series=[("ìš”ì²­ ìˆ˜", counts)],
                    horizontal=True,
                )

        # 9. í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ ì½”ë“œ ë¶„í¬ (ë°” ì°¨íŠ¸)
        client_status = analysis_results.get("client_status_statistics", {})
        if client_status:
            status_totals: dict[str, int] = {}
            for ip_stats in client_status.values():
                if isinstance(ip_stats, dict):
                    for code, count in ip_stats.items():
                        if isinstance(count, int):
                            status_totals[code] = status_totals.get(code, 0) + count
            if status_totals:
                top_codes = sorted(status_totals.items(), key=lambda x: -x[1])[:10]
                if top_codes:
                    codes = [str(code) for code, _ in top_codes]
                    status_counts: list[int | float] = [count for _, count in top_codes]
                    status_series: list[tuple[str, list[int | float]]] = [("ìš”ì²­ ìˆ˜", status_counts)]
                    report.add_bar_chart(
                        "í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ ì½”ë“œ ë¶„í¬",
                        categories=codes,
                        series=status_series,
                    )

        # 10. Targetë³„ ìš”ì²­ ë¶„í¬ (ë°” ì°¨íŠ¸)
        target_request_stats = analysis_results.get("target_request_stats", {})
        if target_request_stats:
            sorted_targets = sorted(
                target_request_stats.items(),
                key=lambda x: x[1].get("total_requests", 0),
                reverse=True,
            )[:10]
            if sorted_targets:
                target_names = [name[:40] + "..." if len(name) > 40 else name for name, _ in sorted_targets]
                request_counts = [stats.get("total_requests", 0) for _, stats in sorted_targets]
                error_counts = [stats.get("error_count", 0) for _, stats in sorted_targets]
                report.add_bar_chart(
                    "Targetë³„ ìš”ì²­ ë¶„í¬",
                    categories=target_names,
                    series=[
                        ("ì •ìƒ ìš”ì²­", [r - e for r, e in zip(request_counts, error_counts, strict=True)]),
                        ("ì—ëŸ¬", error_counts),
                    ],
                    stacked=True,
                    horizontal=True,
                )

        # =============================================================================
        # [ì„±ëŠ¥ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("âš¡ ì„±ëŠ¥ ë¶„ì„")

        # NEW: TPS ì‹œê³„ì—´ ì°¨íŠ¸ (ì²˜ë¦¬ëŸ‰)
        tps_time_series = analysis_results.get("tps_time_series", [])
        if tps_time_series:
            tps_timestamps = [item.get("timestamp") for item in tps_time_series if item.get("timestamp")]
            tps_values = [item.get("tps", 0) for item in tps_time_series]
            if tps_timestamps and any(v > 0 for v in tps_values):
                bucket_minutes = analysis_results.get("bucket_minutes", 15)
                report.add_time_series_chart(
                    f"TPS ì¶”ì´ ({bucket_minutes}ë¶„ ë‹¨ìœ„)",
                    timestamps=tps_timestamps,
                    values={"TPS": tps_values},
                    aggregation="last",
                    area=True,
                )

        # NEW: ì‹œê°„ëŒ€ë³„ Latency ì¶”ì´ ì°¨íŠ¸ (P50/P90/P99)
        if tps_time_series:
            tps_timestamps = [item.get("timestamp") for item in tps_time_series if item.get("timestamp")]
            p50_values = [item.get("p50_ms", 0) for item in tps_time_series]
            p90_values = [item.get("p90_ms", 0) for item in tps_time_series]
            p99_values = [item.get("p99_ms", 0) for item in tps_time_series]
            if tps_timestamps and any(v > 0 for v in p50_values + p90_values + p99_values):
                bucket_minutes = analysis_results.get("bucket_minutes", 15)
                report.add_time_series_chart(
                    f"ì‘ë‹µ ì‹œê°„ ì¶”ì´ ({bucket_minutes}ë¶„ ë‹¨ìœ„)",
                    timestamps=tps_timestamps,
                    values={
                        "P50 (ms)": p50_values,
                        "P90 (ms)": p90_values,
                        "P99 (ms)": p99_values,
                    },
                    aggregation="last",
                    area=False,
                )

        # NEW: SLA ì¤€ìˆ˜ìœ¨ ê²Œì´ì§€ ì°¨íŠ¸ (3ê°œ)
        sla_compliance = analysis_results.get("sla_compliance", {})
        if sla_compliance:
            # 100ms ê¸°ì¤€
            if "under_100ms" in sla_compliance:
                sla_100ms = sla_compliance["under_100ms"]
                report.add_gauge_chart(
                    "SLA: 100ms ë¯¸ë§Œ ì‘ë‹µë¥ ",
                    value=sla_100ms.get("rate", 0),
                    max_value=100,
                    thresholds=[(0.9, "#ee6666"), (0.95, "#fac858"), (1, "#91cc75")],
                    unit="%",
                )

            # 500ms ê¸°ì¤€
            if "under_500ms" in sla_compliance:
                sla_500ms = sla_compliance["under_500ms"]
                report.add_gauge_chart(
                    "SLA: 500ms ë¯¸ë§Œ ì‘ë‹µë¥ ",
                    value=sla_500ms.get("rate", 0),
                    max_value=100,
                    thresholds=[(0.95, "#ee6666"), (0.99, "#fac858"), (1, "#91cc75")],
                    unit="%",
                )

            # 1s ê¸°ì¤€
            if "under_1s" in sla_compliance:
                sla_1s = sla_compliance["under_1s"]
                report.add_gauge_chart(
                    "SLA: 1ì´ˆ ë¯¸ë§Œ ì‘ë‹µë¥ ",
                    value=sla_1s.get("rate", 0),
                    max_value=100,
                    thresholds=[(0.99, "#ee6666"), (0.999, "#fac858"), (1, "#91cc75")],
                    unit="%",
                )

        # 11. ì‘ë‹µ ì‹œê°„ ë°±ë¶„ìœ„ìˆ˜ (ë°” ì°¨íŠ¸)
        response_time_percentiles = analysis_results.get("response_time_percentiles", {})
        if response_time_percentiles:
            percentile_labels = ["P50", "P90", "P95", "P99", "í‰ê· "]
            percentile_values = [
                round(response_time_percentiles.get("p50", 0) * 1000, 1),
                round(response_time_percentiles.get("p90", 0) * 1000, 1),
                round(response_time_percentiles.get("p95", 0) * 1000, 1),
                round(response_time_percentiles.get("p99", 0) * 1000, 1),
                round(response_time_percentiles.get("avg", 0) * 1000, 1),
            ]
            if any(v > 0 for v in percentile_values):
                report.add_bar_chart(
                    "ì‘ë‹µ ì‹œê°„ ë¶„í¬ (ms)",
                    categories=percentile_labels,
                    series=[("ì‘ë‹µ ì‹œê°„", percentile_values)],
                )

        # 12. ì‘ë‹µ ì‹œê°„ êµ¬ê°„ ë¶„í¬ - ë°” ì°¨íŠ¸ (DuckDBì—ì„œ ê³„ì‚°ëœ ë°ì´í„° ìš°ì„  ì‚¬ìš©)
        response_time_distribution = analysis_results.get("response_time_distribution", {})
        long_response_times = analysis_results.get("long_response_times", [])
        if response_time_distribution:
            # DuckDBì—ì„œ ê³„ì‚°ëœ ë¶„í¬ ì‚¬ìš©
            if any(v > 0 for v in response_time_distribution.values()):
                report.add_bar_chart(
                    "ì‘ë‹µ ì‹œê°„ êµ¬ê°„ ë¶„í¬",
                    categories=list(response_time_distribution.keys()),
                    series=[("ìš”ì²­ ìˆ˜", list(response_time_distribution.values()))],
                )
        else:
            # Fallback: long_response_timesì—ì„œ ê³„ì‚°
            if long_response_times or response_time_percentiles:
                # êµ¬ê°„ë³„ ì¹´ìš´íŠ¸ ê³„ì‚°
                buckets = {"<100ms": 0, "100-500ms": 0, "500ms-1s": 0, "1-3s": 0, ">3s": 0}

                # long_response_timesì—ì„œ ì‘ë‹µ ì‹œê°„ ì¶”ì¶œ
                for r in long_response_times:
                    rt = r.get("response_time")
                    if rt is not None:
                        rt_ms = rt * 1000
                        if rt_ms < 100:
                            buckets["<100ms"] += 1
                        elif rt_ms < 500:
                            buckets["100-500ms"] += 1
                        elif rt_ms < 1000:
                            buckets["500ms-1s"] += 1
                        elif rt_ms < 3000:
                            buckets["1-3s"] += 1
                        else:
                            buckets[">3s"] += 1

                # ê°’ì´ ìˆìœ¼ë©´ ì°¨íŠ¸ ì¶”ê°€
                if any(v > 0 for v in buckets.values()):
                    report.add_bar_chart(
                        "ì‘ë‹µ ì‹œê°„ êµ¬ê°„ ë¶„í¬",
                        categories=list(buckets.keys()),
                        series=[("ìš”ì²­ ìˆ˜", list(buckets.values()))],
                    )

        # 13. ë°ì´í„° ì „ì†¡ëŸ‰ (ë°” ì°¨íŠ¸)
        total_received = analysis_results.get("total_received_bytes") or 0
        total_sent = analysis_results.get("total_sent_bytes") or 0
        if total_received > 0 or total_sent > 0:

            def to_gb(b: int) -> float:
                return b / (1024**3) if b else 0

            transfer_data = [
                ("ìˆ˜ì‹  (Received)", to_gb(total_received)),
                ("ì†¡ì‹  (Sent)", to_gb(total_sent)),
            ]
            transfer_data = [(name, val) for name, val in transfer_data if val > 0]
            if transfer_data:
                report.add_bar_chart(
                    "ë°ì´í„° ì „ì†¡ëŸ‰ (GB)",
                    categories=[name for name, _ in transfer_data],
                    series=[("GB", [round(val, 2) for _, val in transfer_data])],
                )

        # 14. Backend ìƒíƒœ ì½”ë“œ ë¶„í¬ (ë„ë„› ì°¨íŠ¸)
        backend_status_data = [
            ("4xx í´ë¼ì´ì–¸íŠ¸ ì—ëŸ¬", backend_4xx),
            ("5xx ì„œë²„ ì—ëŸ¬", backend_5xx),
        ]
        backend_status_data = [(name, count) for name, count in backend_status_data if count > 0]
        if backend_status_data:
            report.add_pie_chart("Backend ìƒíƒœ ì½”ë“œ ë¶„í¬", backend_status_data, doughnut=True)

        # NEW: Targetë³„ Latency ë¹„êµ ì°¨íŠ¸
        target_latency_stats = analysis_results.get("target_latency_stats", {})
        if target_latency_stats:
            # Top 10 targets by request count
            sorted_targets = sorted(
                target_latency_stats.items(),
                key=lambda x: x[1].get("total_requests", 0),
                reverse=True,
            )[:10]
            if sorted_targets:
                target_names = [name[:40] + "..." if len(name) > 40 else name for name, _ in sorted_targets]
                p50_values = [stats.get("p50_ms", 0) for _, stats in sorted_targets]
                p90_values = [stats.get("p90_ms", 0) for _, stats in sorted_targets]
                p99_values = [stats.get("p99_ms", 0) for _, stats in sorted_targets]
                report.add_bar_chart(
                    "Targetë³„ ì‘ë‹µ ì‹œê°„ ë¹„êµ (ms)",
                    categories=target_names,
                    series=[
                        ("P50", p50_values),
                        ("P90", p90_values),
                        ("P99", p99_values),
                    ],
                    horizontal=True,
                )

        # NEW: ì²˜ë¦¬ ì‹œê°„ ë¶„í•´ ë¶„ì„ (Request/Target/Response)
        processing_time_breakdown = analysis_results.get("processing_time_breakdown", {})
        if processing_time_breakdown:
            req_data = processing_time_breakdown.get("request", {})
            target_data = processing_time_breakdown.get("target", {})
            resp_data = processing_time_breakdown.get("response", {})

            if any([req_data, target_data, resp_data]):
                # P50/P90/P99ë³„ ê° ë‹¨ê³„ì˜ ì²˜ë¦¬ ì‹œê°„
                report.add_bar_chart(
                    "ì²˜ë¦¬ ë‹¨ê³„ë³„ ì‘ë‹µ ì‹œê°„ (ms)",
                    categories=["P50", "P90", "P99"],
                    series=[
                        (
                            "Request (ALB)",
                            [req_data.get("p50_ms", 0), req_data.get("p90_ms", 0), req_data.get("p99_ms", 0)],
                        ),
                        (
                            "Target (Backend)",
                            [target_data.get("p50_ms", 0), target_data.get("p90_ms", 0), target_data.get("p99_ms", 0)],
                        ),
                        (
                            "Response (ì „ì†¡)",
                            [resp_data.get("p50_ms", 0), resp_data.get("p90_ms", 0), resp_data.get("p99_ms", 0)],
                        ),
                    ],
                )

        # NEW: ì—°ê²° ì‹¤íŒ¨ ë¶„ì„
        connection_failures = analysis_results.get("connection_failures", {})
        if connection_failures and connection_failures.get("target_failures", 0) > 0:
            failure_data = [
                ("Request ì‹¤íŒ¨", connection_failures.get("request_failures", 0)),
                ("Target ì—°ê²° ì‹¤íŒ¨", connection_failures.get("target_failures", 0)),
                ("Response ì „ì†¡ ì‹¤íŒ¨", connection_failures.get("response_failures", 0)),
            ]
            failure_data = [(name, count) for name, count in failure_data if count > 0]
            if failure_data:
                report.add_pie_chart("ì—°ê²° ì‹¤íŒ¨ ìœ í˜• ë¶„í¬", failure_data, doughnut=True)

            # Targetë³„ ì‹¤íŒ¨ ìƒì„¸
            target_failures_detail = connection_failures.get("target_failures_detail", [])
            if target_failures_detail:
                target_names = [
                    f"{d['target_group']}({d['target']})" if d["target_group"] else d["target"]
                    for d in target_failures_detail[:10]
                ]
                failure_counts = [d["count"] for d in target_failures_detail[:10]]
                report.add_bar_chart(
                    "Targetë³„ ì—°ê²° ì‹¤íŒ¨",
                    categories=target_names,
                    series=[("ì‹¤íŒ¨ ìˆ˜", failure_counts)],
                    horizontal=True,
                )

        # ëŠë¦° ì‘ë‹µ í…Œì´ë¸” (ì„±ëŠ¥ ë¶„ì„ ì„¹ì…˜)
        if long_response_times:
            slow_rows = []
            for r in long_response_times[:100]:
                response_time = r.get("response_time")
                response_time_str = f"{response_time:.3f}" if response_time is not None else "-"
                slow_rows.append(
                    [
                        str(r.get("timestamp") or "")[:19],
                        r.get("client_ip") or "",
                        (r.get("request") or "")[:50],
                        response_time_str,
                        r.get("elb_status_code") or "",
                        r.get("target_status_code") or "",
                    ]
                )
            report.add_table(
                title="ğŸ¢ ëŠë¦° ì‘ë‹µ Top 100",
                headers=["ì‹œê°„", "Client IP", "URL", "ì‘ë‹µ ì‹œê°„(s)", "ELB Status", "Target Status"],
                rows=slow_rows,
                page_size=20,
            )

        # =============================================================================
        # [í”„ë¡œí† ì½œ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸ” í”„ë¡œí† ì½œ ë¶„ì„")

        # NEW: HTTP ë²„ì „ ë¶„í¬
        http_version_distribution = analysis_results.get("http_version_distribution", {})
        if http_version_distribution:
            http_version_data = [(version, count) for version, count in http_version_distribution.items() if count > 0]
            if http_version_data:
                report.add_pie_chart("HTTP í”„ë¡œí† ì½œ ë²„ì „ ë¶„í¬", http_version_data, doughnut=True)

        # NEW: SSL/TLS í”„ë¡œí† ì½œ ë¶„í¬
        ssl_stats = analysis_results.get("ssl_stats", {})
        if ssl_stats:
            protocol_dist = ssl_stats.get("protocol_distribution", {})
            if protocol_dist:
                tls_data = [(proto, count) for proto, count in protocol_dist.items() if count > 0]
                if tls_data:
                    report.add_pie_chart("TLS í”„ë¡œí† ì½œ ë²„ì „ ë¶„í¬", tls_data, doughnut=True)

            cipher_dist = ssl_stats.get("cipher_distribution", {})
            if cipher_dist:
                cipher_names = list(cipher_dist.keys())[:10]
                cipher_counts = [cipher_dist[c] for c in cipher_names]
                if cipher_names:
                    report.add_bar_chart(
                        "ì•”í˜¸ ìŠ¤ìœ„íŠ¸ Top 10",
                        categories=cipher_names,
                        series=[("ìš”ì²­ ìˆ˜", cipher_counts)],
                        horizontal=True,
                    )

            # ì·¨ì•½ TLS ê²½ê³ 
            weak_tls_clients = ssl_stats.get("weak_tls_clients", [])
            if weak_tls_clients:
                report.add_table(
                    title=f"âš ï¸ ì·¨ì•½ TLS ì‚¬ìš© í´ë¼ì´ì–¸íŠ¸ ({len(weak_tls_clients)}ê°œ)",
                    headers=["í´ë¼ì´ì–¸íŠ¸ IP", "TLS ë²„ì „", "ìš”ì²­ ìˆ˜"],
                    rows=[[c["client_ip"], c["protocol"], c["count"]] for c in weak_tls_clients[:20]],
                    page_size=10,
                )

        # NEW: Actions ë¶„í¬
        actions_stats = analysis_results.get("actions_stats", {})
        if actions_stats:
            # 'None'ê³¼ 'Forward'ë¥¼ ì œì™¸í•œ ì˜ë¯¸ ìˆëŠ” ì•¡ì…˜ë§Œ í‘œì‹œ
            meaningful_actions = [
                (action, count)
                for action, count in actions_stats.items()
                if action not in ("None", "Forward", "Other") and count > 0
            ]
            if meaningful_actions:
                report.add_pie_chart("ALB Actions ë¶„í¬ (WAF/ì¸ì¦/ë¦¬ë‹¤ì´ë ‰íŠ¸)", meaningful_actions, doughnut=True)

        # =============================================================================
        # [ë³´ì•ˆ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸ›¡ï¸ ë³´ì•ˆ ë¶„ì„")

        # NEW: Classification ë¶„í¬ (Desync íƒì§€)
        classification_stats = analysis_results.get("classification_stats", {})
        if classification_stats:
            class_dist = classification_stats.get("distribution", {})
            if class_dist:
                # Ambiguous/Severeê°€ ìˆìœ¼ë©´ ê²½ê³ 
                ambiguous_count = class_dist.get("Ambiguous", 0)
                severe_count = class_dist.get("Severe", 0)

                if ambiguous_count > 0 or severe_count > 0:
                    class_data = [(cls, count) for cls, count in class_dist.items() if count > 0]
                    report.add_pie_chart("HTTP ìš”ì²­ ë¶„ë¥˜ (Desync íƒì§€)", class_data, doughnut=True)

            # ë³´ì•ˆ ì´ë²¤íŠ¸ í…Œì´ë¸” (ì „ì²´ í‘œì‹œ)
            security_events = classification_stats.get("security_events", [])
            if security_events:
                report.add_table(
                    title=f"ğŸš¨ ë³´ì•ˆ ì´ë²¤íŠ¸ (Ambiguous/Severe ìš”ì²­) - {len(security_events)}ê±´",
                    headers=["ì‹œê°„", "í´ë¼ì´ì–¸íŠ¸ IP", "ë¶„ë¥˜", "ì‚¬ìœ ", "URL"],
                    rows=[
                        [
                            e["timestamp"].strftime("%Y-%m-%d %H:%M:%S")
                            if hasattr(e["timestamp"], "strftime")
                            else str(e["timestamp"]),
                            e["client_ip"],
                            e["classification"],
                            e["reason"][:30] + "..." if len(e.get("reason", "")) > 30 else e.get("reason", ""),
                            e["url"][:50] + "..." if len(e.get("url", "")) > 50 else e.get("url", ""),
                        ]
                        for e in security_events
                    ],
                    page_size=50,
                )

        # ì•…ì„± IP í…Œì´ë¸” (ë³´ì•ˆ ë¶„ì„ ì„¹ì…˜)
        if matching_abuse_ips:
            report.add_table(
                title=f"ğŸš« ì•…ì„± IP ëª©ë¡ ({len(matching_abuse_ips)}ê°œ)",
                headers=["IP ì£¼ì†Œ", "ìš”ì²­ ìˆ˜", "ìƒíƒœ"],
                rows=[[ip, count, "AbuseIPDB ë“±ë¡"] for ip, count in matching_abuse_ips[:50]],
                page_size=20,
            )

        # =============================================================================
        # [ì—ëŸ¬ ë¶„ì„ ì„¹ì…˜]
        # =============================================================================
        report.add_section_title("ğŸš¨ ì—ëŸ¬ ë¶„ì„")

        # 15. ELB vs Backend ì—ëŸ¬ ë¹„êµ (ë°” ì°¨íŠ¸)
        if elb_4xx > 0 or elb_5xx > 0 or backend_4xx > 0 or backend_5xx > 0:
            report.add_bar_chart(
                "ELB vs Backend ì—ëŸ¬ ë¹„êµ",
                categories=["4xx ì—ëŸ¬", "5xx ì—ëŸ¬"],
                series=[
                    ("ELB", [elb_4xx, elb_5xx]),
                    ("Backend", [backend_4xx, backend_5xx]),
                ],
            )

        # 16. ì—ëŸ¬ ì›ì¸ ë¶„í¬ (ë°” ì°¨íŠ¸)
        error_reason_counts = analysis_results.get("error_reason_counts", {})
        if error_reason_counts:
            sorted_reasons = sorted(error_reason_counts.items(), key=lambda x: -x[1])[:10]
            if sorted_reasons:
                reasons = [reason for reason, _ in sorted_reasons]
                counts = [count for _, count in sorted_reasons]
                report.add_bar_chart(
                    "ì—ëŸ¬ ì›ì¸ ë¶„í¬",
                    categories=reasons,
                    series=[("ê±´ìˆ˜", counts)],
                    horizontal=True,
                )

        # 17. URLë³„ ì—ëŸ¬ìœ¨ (ë°” ì°¨íŠ¸)
        url_error_stats = analysis_results.get("url_error_stats", {})
        if url_error_stats:
            sorted_urls = sorted(
                [(url, stats) for url, stats in url_error_stats.items() if stats.get("error_count", 0) > 0],
                key=lambda x: x[1].get("error_rate", 0),
                reverse=True,
            )[:15]
            if sorted_urls:
                url_names = [url[:50] + "..." if len(url) > 50 else url for url, _ in sorted_urls]
                error_rates = [stats.get("error_rate", 0) for _, stats in sorted_urls]
                report.add_bar_chart(
                    "URLë³„ ì—ëŸ¬ìœ¨ (%)",
                    categories=url_names,
                    series=[("ì—ëŸ¬ìœ¨", error_rates)],
                    horizontal=True,
                )

        # 18. ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ìœ¨ ì¶”ì´ (NEW) - ë¼ì¸ ì°¨íŠ¸
        error_timestamps: list[datetime] = []
        error_types: list[str] = []

        for key, error_type in [("ELB 4xx Count", "4xx"), ("ELB 5xx Count", "5xx")]:
            log_data = analysis_results.get(key, {})
            if isinstance(log_data, dict):
                timestamps = log_data.get("timestamps", [])
                for ts in timestamps:
                    if ts and hasattr(ts, "timestamp"):
                        error_timestamps.append(ts)
                        error_types.append(error_type)

        # ì—ëŸ¬ìœ¨ ì¶”ì´ ê³„ì‚° (ì‹œê°„ ë²„í‚·ë³„)
        if all_timestamps and error_timestamps:
            from collections import defaultdict

            # ë²„í‚· í¬ê¸° ê²°ì • (ì‹œê°„ ë²”ìœ„ì— ë”°ë¼)
            min_time = min(all_timestamps) if all_timestamps else datetime.now()
            max_time = max(all_timestamps) if all_timestamps else datetime.now()
            total_seconds = (max_time - min_time).total_seconds()
            total_hours = total_seconds / 3600

            if total_hours <= 3:
                bucket_minutes = 5
            elif total_hours <= 24:
                bucket_minutes = 15
            elif total_hours <= 24 * 7:
                bucket_minutes = 60
            else:
                bucket_minutes = 240

            bucket_seconds = bucket_minutes * 60

            # ë²„í‚·ë³„ ì „ì²´ ìš”ì²­ê³¼ ì—ëŸ¬ ì¹´ìš´íŠ¸
            total_buckets: dict[datetime, int] = defaultdict(int)
            error_buckets: dict[datetime, int] = defaultdict(int)

            for ts in all_timestamps:
                bucket_start = datetime.fromtimestamp((ts.timestamp() // bucket_seconds) * bucket_seconds)
                total_buckets[bucket_start] += 1

            for ts in error_timestamps:
                bucket_start = datetime.fromtimestamp((ts.timestamp() // bucket_seconds) * bucket_seconds)
                error_buckets[bucket_start] += 1

            # ì—ëŸ¬ìœ¨ ê³„ì‚°
            if total_buckets:
                sorted_buckets = sorted(total_buckets.keys())
                error_rates_time: list[float] = []

                for bucket in sorted_buckets:
                    total = total_buckets[bucket]
                    errors = error_buckets.get(bucket, 0)
                    rate = (errors / total * 100) if total > 0 else 0
                    error_rates_time.append(round(rate, 2))

                # ì‹œê°„ í¬ë§·
                if total_hours <= 24:
                    time_format = "%H:%M"
                elif total_hours <= 24 * 7:
                    time_format = "%m/%d %H:%M"
                else:
                    time_format = "%m/%d"

                categories = [ts.strftime(time_format) for ts in sorted_buckets]

                if any(r > 0 for r in error_rates_time):
                    report.add_line_chart(
                        f"ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ìœ¨ ì¶”ì´ ({bucket_minutes}ë¶„ ë‹¨ìœ„)",
                        categories=categories,
                        series=[("ì—ëŸ¬ìœ¨ (%)", error_rates_time)],
                        area=True,
                        smooth=True,
                    )

        # 19. ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ íŠ¸ë Œë“œ (ë¼ì¸ ì°¨íŠ¸)
        if error_timestamps:
            is_4xx: list[int | float] = [1 if t == "4xx" else 0 for t in error_types]
            is_5xx: list[int | float] = [1 if t == "5xx" else 0 for t in error_types]

            error_values: dict[str, list[int | float]] = {
                "4xx ì—ëŸ¬": is_4xx,
                "5xx ì—ëŸ¬": is_5xx,
            }
            report.add_time_series_chart(
                "ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ íŠ¸ë Œë“œ",
                timestamps=error_timestamps,
                values=error_values,
                aggregation="sum",
                area=True,
            )

        # =============================================================================
        # í…Œì´ë¸” ì„¹ì…˜
        # =============================================================================
        def format_bytes(b: int | None) -> str:
            if b is None:
                return "0 B"
            if b >= 1024**3:
                return f"{b / 1024**3:.2f} GB"
            elif b >= 1024**2:
                return f"{b / 1024**2:.2f} MB"
            elif b >= 1024:
                return f"{b / 1024:.2f} KB"
            return f"{b} B"

        # ë¶„ì„ ì •ë³´ í…Œì´ë¸”
        report.add_table(
            title="ë¶„ì„ ì •ë³´",
            headers=["í•­ëª©", "ê°’"],
            rows=[
                ["S3 ê²½ë¡œ", analysis_results.get("s3_uri", "")],
                [
                    "ë¶„ì„ ê¸°ê°„ (ìš”ì²­)",
                    f"{analysis_results.get('start_time', '')} ~ {analysis_results.get('end_time', '')}",
                ],
                [
                    "ì‹¤ì œ ë°ì´í„° ê¸°ê°„",
                    f"{analysis_results.get('actual_start_time', '')} ~ {analysis_results.get('actual_end_time', '')}",
                ],
                ["íƒ€ì„ì¡´", analysis_results.get("timezone", "")],
                ["ì´ ë¡œê·¸ ë¼ì¸", f"{total_logs:,}"],
                ["ë¡œê·¸ íŒŒì¼ ìˆ˜", f"{analysis_results.get('log_files_count') or 0:,}"],
                ["ìˆ˜ì‹  ë°ì´í„°", format_bytes(total_received)],
                ["ì†¡ì‹  ë°ì´í„°", format_bytes(total_sent)],
            ],
            sortable=False,
            searchable=False,
        )

        # íŒŒì¼ëª… ìƒì„± ë° ì €ì¥
        report_filename = _generate_report_filename(analyzer, analysis_results).replace(".xlsx", ".html")
        html_path = os.path.join(output_dir, report_filename)

        report.save(html_path, auto_open=False)
        return html_path

    except Exception as e:
        console.print(f"[yellow]âš ï¸ HTML ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}[/yellow]")
        import traceback

        traceback.print_exc()
        return None


def _select_alb_with_pagination(
    alb_list: list[dict[str, Any]],
    page_size: int = 20,
) -> dict[str, Any] | None:
    """í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ALB ì„ íƒ

    Args:
        alb_list: ALB ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{"lb": ..., "name": ..., "scheme": ..., "status": ...}, ...]
        page_size: í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜ (ê¸°ë³¸ 20)

    Returns:
        ì„ íƒëœ ALBì˜ lb ê°ì²´ ë˜ëŠ” None

    Raises:
        KeyboardInterrupt: ì‚¬ìš©ìê°€ ì·¨ì†Œí•œ ê²½ìš°
    """
    if not alb_list:
        return None

    total = len(alb_list)
    (total + page_size - 1) // page_size
    current_page = 0
    filtered_list = alb_list  # ê²€ìƒ‰ í•„í„°ë§ëœ ë¦¬ìŠ¤íŠ¸

    while True:
        # í˜„ì¬ í˜ì´ì§€ í•­ëª© ê³„ì‚°
        start_idx = current_page * page_size
        end_idx = min(start_idx + page_size, len(filtered_list))
        page_items = filtered_list[start_idx:end_idx]

        # í…Œì´ë¸” ì¶œë ¥
        table = Table(
            title=f"[bold cyan]ALB ëª©ë¡[/bold cyan] (í˜ì´ì§€ {current_page + 1}/{max(1, (len(filtered_list) + page_size - 1) // page_size)}, ì´ {len(filtered_list)}ê°œ)",
            show_header=True,
            header_style="bold blue",
        )
        table.add_column("No.", style="dim", width=5, justify="right")
        table.add_column("ALB ì´ë¦„", style="cyan", min_width=30)
        table.add_column("Scheme", width=16, justify="center")
        table.add_column("ë¡œê·¸", width=4, justify="center")

        for i, item in enumerate(page_items, start=start_idx + 1):
            table.add_row(
                str(i),
                item["name"],
                item["scheme"],
                item["status"],
            )

        console.print()
        console.print(table)

        # ë„¤ë¹„ê²Œì´ì…˜ ì•ˆë‚´
        nav_hints = []
        if current_page > 0:
            nav_hints.append("[dim]p: ì´ì „[/dim]")
        if end_idx < len(filtered_list):
            nav_hints.append("[dim]n: ë‹¤ìŒ[/dim]")
        nav_hints.append("[dim]/ê²€ìƒ‰ì–´: ê²€ìƒ‰[/dim]")
        nav_hints.append("[dim]q: ì·¨ì†Œ[/dim]")

        console.print(" | ".join(nav_hints))

        # ì…ë ¥ ë°›ê¸°
        try:
            user_input = questionary.text(
                "ë²ˆí˜¸ ì…ë ¥ ë˜ëŠ” ëª…ë ¹:",
            ).ask()
        except KeyboardInterrupt:
            raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.") from None

        if user_input is None:
            raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.") from None

        user_input = user_input.strip()

        # ë¹ˆ ì…ë ¥ ë¬´ì‹œ
        if not user_input:
            continue

        # ëª…ë ¹ì–´ ì²˜ë¦¬
        if user_input.lower() == "q":
            raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.") from None

        if user_input.lower() == "n":
            if end_idx < len(filtered_list):
                current_page += 1
            else:
                console.print("[yellow]ë§ˆì§€ë§‰ í˜ì´ì§€ì…ë‹ˆë‹¤.[/yellow]")
            continue

        if user_input.lower() == "p":
            if current_page > 0:
                current_page -= 1
            else:
                console.print("[yellow]ì²« ë²ˆì§¸ í˜ì´ì§€ì…ë‹ˆë‹¤.[/yellow]")
            continue

        # ê²€ìƒ‰ ì²˜ë¦¬ (/ë¡œ ì‹œì‘)
        if user_input.startswith("/"):
            search_term = user_input[1:].strip().lower()
            if search_term:
                filtered_list = [item for item in alb_list if search_term in item["name"].lower()]
                current_page = 0
                if not filtered_list:
                    console.print(f"[yellow]'{search_term}' ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ëª©ë¡ìœ¼ë¡œ ë³µì›í•©ë‹ˆë‹¤.[/yellow]")
                    filtered_list = alb_list
                else:
                    console.print(f"[green]'{search_term}' ê²€ìƒ‰ ê²°ê³¼: {len(filtered_list)}ê°œ[/green]")
            else:
                # ë¹ˆ ê²€ìƒ‰ì–´ëŠ” ì „ì²´ ëª©ë¡ ë³µì›
                filtered_list = alb_list
                current_page = 0
                console.print("[green]ì „ì²´ ëª©ë¡ìœ¼ë¡œ ë³µì›í•©ë‹ˆë‹¤.[/green]")
            continue

        # ë²ˆí˜¸ ì…ë ¥ ì²˜ë¦¬
        try:
            selected_num = int(user_input)
            if 1 <= selected_num <= len(filtered_list):
                selected_item = filtered_list[selected_num - 1]
                console.print(f"[green]âœ“ ì„ íƒë¨: {selected_item['name']}[/green]")
                return dict(selected_item["lb"])
            else:
                console.print(f"[red]1~{len(filtered_list)} ë²”ìœ„ì˜ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/red]")
        except ValueError:
            console.print("[yellow]ë²ˆí˜¸, ëª…ë ¹ì–´(n/p/q), ë˜ëŠ” /ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.[/yellow]")


def _get_bucket_input_with_options(session, ctx) -> str | None:
    """S3 ë²„í‚· ê²½ë¡œ ì…ë ¥ ë°©ì‹ ì„ íƒ

    Returns:
        S3 ë²„í‚· ê²½ë¡œ ë˜ëŠ” None (ì·¨ì†Œ ì‹œ)

    Raises:
        KeyboardInterrupt: ì‚¬ìš©ìê°€ ì·¨ì†Œí•œ ê²½ìš°
    """
    choices = [
        questionary.Choice("ALB ë¡œê·¸ ê²½ë¡œ ìë™ íƒìƒ‰", value="auto"),
        questionary.Choice("ALB ë¡œê·¸ ê²½ë¡œ ìˆ˜ë™ ì…ë ¥", value="manual"),
    ]

    choice = questionary.select(
        "S3 ë²„í‚· ê²½ë¡œ ì…ë ¥ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:",
        choices=choices,
    ).ask()

    if choice is None:
        raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")

    if choice == "auto":
        return _get_lb_and_build_path(session, ctx)
    else:
        return _get_bucket_input_manual()


def _get_lb_and_build_path(session, ctx) -> str | None:
    """ìë™ íƒìƒ‰ìœ¼ë¡œ S3 ê²½ë¡œ ìƒì„±"""
    from botocore.exceptions import ClientError

    elbv2_client = get_client(session, "elbv2")

    # ALB ëª©ë¡ ì¡°íšŒ
    try:
        console.print("[cyan]Application Load Balancer ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” ì¤‘...[/cyan]")
        response = elbv2_client.describe_load_balancers()

        albs = [lb for lb in response["LoadBalancers"] if lb["Type"] == "application"]

        if not albs:
            console.print("[yellow]! ì´ ê³„ì •ì— ALBê°€ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ ì…ë ¥ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.[/yellow]")
            return _get_bucket_input_manual()

        console.print(f"[green]âœ“ {len(albs)}ê°œì˜ ALBë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.[/green]")

    except ClientError as e:
        if "AccessDenied" in str(e):
            console.print("[yellow]! ELB API ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ ì…ë ¥ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.[/yellow]")
        else:
            console.print(f"[yellow]! ALB ì¡°íšŒ ì‹¤íŒ¨: {e}. ìˆ˜ë™ ì…ë ¥ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.[/yellow]")
        return _get_bucket_input_manual()

    # ALB ì„ íƒ - ëª©ë¡ ìƒì„±
    alb_list: list[dict[str, Any]] = []

    for lb in sorted(albs, key=lambda x: x["LoadBalancerName"]):
        # ë¡œê·¸ ì„¤ì • í™•ì¸
        try:
            attrs = elbv2_client.describe_load_balancer_attributes(LoadBalancerArn=lb["LoadBalancerArn"])
            log_enabled = any(
                attr["Key"] == "access_logs.s3.enabled" and attr["Value"] == "true" for attr in attrs["Attributes"]
            )
            status = "[green]âœ“[/green]" if log_enabled else "[red]âœ—[/red]"
        except Exception as e:
            logger.debug("Failed to get ALB log status: %s", e)
            status = "[dim]?[/dim]"

        alb_list.append(
            {
                "lb": lb,
                "name": lb["LoadBalancerName"],
                "scheme": lb["Scheme"],
                "status": status,
            }
        )

    # í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ALB ì„ íƒ
    selected_lb = _select_alb_with_pagination(alb_list)

    if not selected_lb:
        return _get_bucket_input_manual()

    # ë¡œê·¸ ì„¤ì • í™•ì¸
    try:
        attrs = elbv2_client.describe_load_balancer_attributes(LoadBalancerArn=selected_lb["LoadBalancerArn"])

        log_config = {}
        for attr in attrs["Attributes"]:
            if attr["Key"] == "access_logs.s3.enabled":
                log_config["enabled"] = attr["Value"] == "true"
            elif attr["Key"] == "access_logs.s3.bucket":
                log_config["bucket"] = attr["Value"]
            elif attr["Key"] == "access_logs.s3.prefix":
                log_config["prefix"] = attr["Value"]

        if not log_config.get("enabled"):
            console.print(
                f"[yellow]âš ï¸ '{selected_lb['LoadBalancerName']}'ì˜ ì•¡ì„¸ìŠ¤ ë¡œê·¸ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.[/yellow]"
            )
            return _get_bucket_input_manual()

        if not log_config.get("bucket"):
            console.print(f"[yellow]âš ï¸ '{selected_lb['LoadBalancerName']}'ì˜ ë¡œê·¸ ë²„í‚· ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            return _get_bucket_input_manual()

        # S3 ê²½ë¡œ ìƒì„±
        bucket_name = log_config["bucket"]
        prefix = log_config.get("prefix", "")

        # ê³„ì • ID ì¶”ì¶œ
        try:
            sts = get_client(session, "sts")
            account_id = sts.get_caller_identity()["Account"]
        except Exception as e:
            logger.debug("Failed to get account ID: %s", e)
            account_id = "unknown"

        # ë¦¬ì „ ì¶”ì¶œ
        region = selected_lb["AvailabilityZones"][0]["ZoneName"][:-1]

        # S3 ê²½ë¡œ ìƒì„±
        if prefix:
            s3_path = f"s3://{bucket_name}/{prefix}/AWSLogs/{account_id}/elasticloadbalancing/{region}/"
        else:
            s3_path = f"s3://{bucket_name}/AWSLogs/{account_id}/elasticloadbalancing/{region}/"

        console.print(f"[green]âœ“ ìë™ ìƒì„±ëœ S3 ê²½ë¡œ: {s3_path}[/green]")
        return s3_path

    except ClientError as e:
        console.print(f"[yellow]! ë¡œê·¸ ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: {e}. ìˆ˜ë™ ì…ë ¥ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.[/yellow]")
        return _get_bucket_input_manual()


def _get_bucket_input_manual() -> str | None:
    """ìˆ˜ë™ìœ¼ë¡œ S3 ë²„í‚· ê²½ë¡œ ì…ë ¥

    Returns:
        S3 ë²„í‚· ê²½ë¡œ ë˜ëŠ” None (ì·¨ì†Œ ì‹œ)
    """
    console.print(
        Panel(
            "[bold cyan]S3 ë²„í‚· ê²½ë¡œ í˜•ì‹:[/bold cyan]\n"
            "s3://bucket-name/prefix\n\n"
            "[bold cyan]ì˜ˆì‹œ:[/bold cyan]\n"
            "s3://my-alb-logs/AWSLogs/123456789012/elasticloadbalancing/ap-northeast-2",
            title="[bold]ë²„í‚· ê²½ë¡œ ì•ˆë‚´[/bold]",
        )
    )

    while True:
        bucket = questionary.text(
            "S3 ë²„í‚· ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (s3://...):",
        ).ask()

        # Ctrl+C ë˜ëŠ” ESCë¡œ ì·¨ì†Œí•œ ê²½ìš°
        if bucket is None:
            raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")

        if not bucket.strip():
            console.print("[red]S3 ë²„í‚· ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.[/red]")
            continue

        if not bucket.startswith("s3://"):
            bucket = f"s3://{bucket}"

        # ê¸°ë³¸ ê²€ì¦
        parts = bucket.split("/")
        if len(parts) < 3 or not parts[2]:
            console.print("[red]ìœ íš¨í•˜ì§€ ì•Šì€ S3 ê²½ë¡œì…ë‹ˆë‹¤.[/red]")
            continue

        # í•„ìˆ˜ ê²½ë¡œ í™•ì¸
        required = ["/AWSLogs/", "/elasticloadbalancing/"]
        missing = [p for p in required if p not in bucket]
        if missing:
            console.print(f"[yellow]âš ï¸ í•„ìˆ˜ ê²½ë¡œê°€ ëˆ„ë½ë¨: {', '.join(missing)}[/yellow]")
            confirm = questionary.confirm("ê·¸ë˜ë„ ì´ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", default=False).ask()
            if confirm is None:
                raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
            if not confirm:
                continue

        return str(bucket)


def _get_time_range_input() -> tuple[datetime, datetime]:
    """ì‹œê°„ ë²”ìœ„ ì…ë ¥

    Raises:
        KeyboardInterrupt: ì‚¬ìš©ìê°€ ì·¨ì†Œí•œ ê²½ìš°
    """
    now = datetime.now()
    yesterday = now - timedelta(days=1)

    console.print("\n[bold cyan]ë¶„ì„ ì‹œê°„ ë²”ìœ„ ì„¤ì •[/bold cyan]")
    console.print(f"[dim]ê¸°ë³¸ê°’: {yesterday.strftime('%Y-%m-%d %H:%M')} ~ {now.strftime('%Y-%m-%d %H:%M')}[/dim]")

    # ë¹ ë¥¸ ì„ íƒ (ê¸°ë³¸ê°’ì¸ 24ì‹œê°„ì„ ì²« ë²ˆì§¸ì— ë°°ì¹˜)
    quick_choices = [
        questionary.Choice("ìµœê·¼ 24ì‹œê°„", value="24h"),
        questionary.Choice("ìµœê·¼ 1ì‹œê°„", value="1h"),
        questionary.Choice("ìµœê·¼ 6ì‹œê°„", value="6h"),
        questionary.Choice("ìµœê·¼ 7ì¼", value="7d"),
        questionary.Choice("ì§ì ‘ ì…ë ¥", value="custom"),
    ]

    choice = questionary.select(
        "ì‹œê°„ ë²”ìœ„ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        choices=quick_choices,
    ).ask()

    if choice is None:
        raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")

    if choice == "custom":
        # ì§ì ‘ ì…ë ¥
        start_str = questionary.text(
            "ì‹œì‘ ì‹œê°„ (YYYY-MM-DD HH:MM):",
        ).ask()
        if start_str is None:
            raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")

        end_str = questionary.text(
            "ì¢…ë£Œ ì‹œê°„ (YYYY-MM-DD HH:MM):",
        ).ask()
        if end_str is None:
            raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")

        try:
            start_time = datetime.strptime(start_str, "%Y-%m-%d %H:%M")
            end_time = datetime.strptime(end_str, "%Y-%m-%d %H:%M")
        except ValueError:
            console.print("[yellow]âš ï¸ ì˜ëª»ëœ í˜•ì‹. ê¸°ë³¸ê°’(24ì‹œê°„)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.[/yellow]")
            start_time = yesterday
            end_time = now
    else:
        # ë¹ ë¥¸ ì„ íƒ
        time_deltas = {
            "1h": timedelta(hours=1),
            "6h": timedelta(hours=6),
            "24h": timedelta(days=1),
            "7d": timedelta(days=7),
        }
        delta = time_deltas.get(choice, timedelta(days=1))
        start_time = now - delta
        end_time = now

    console.print(
        f"[green]âœ“ ë¶„ì„ ê¸°ê°„: {start_time.strftime('%Y-%m-%d %H:%M')} ~ {end_time.strftime('%Y-%m-%d %H:%M')}[/green]"
    )
    return start_time, end_time


def _get_timezone_input() -> str:
    """íƒ€ì„ì¡´ ì…ë ¥

    Raises:
        KeyboardInterrupt: ì‚¬ìš©ìê°€ ì·¨ì†Œí•œ ê²½ìš°
    """
    tz_choices = [
        questionary.Choice("Asia/Seoul (í•œêµ­)", value="Asia/Seoul"),
        questionary.Choice("UTC", value="UTC"),
        questionary.Choice("America/New_York", value="America/New_York"),
        questionary.Choice("Europe/London", value="Europe/London"),
        questionary.Choice("ì§ì ‘ ì…ë ¥", value="custom"),
    ]

    choice = questionary.select(
        "íƒ€ì„ì¡´ì„ ì„ íƒí•˜ì„¸ìš”:",
        choices=tz_choices,
    ).ask()

    if choice is None:
        raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")

    if choice == "custom":
        tz = questionary.text("íƒ€ì„ì¡´ ì…ë ¥:", default="Asia/Seoul").ask()
        if tz is None:
            raise KeyboardInterrupt("ì‚¬ìš©ìê°€ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
        try:
            pytz.timezone(tz)
            return str(tz)
        except pytz.exceptions.UnknownTimeZoneError:
            console.print("[yellow]âš ï¸ ì˜ëª»ëœ íƒ€ì„ì¡´. Asia/Seoulì„ ì‚¬ìš©í•©ë‹ˆë‹¤.[/yellow]")
            return "Asia/Seoul"

    return str(choice)


def _create_output_directory(ctx) -> str:
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    from core.tools.output import OutputPath

    # identifier ê²°ì •
    if ctx.is_sso_session() and ctx.accounts:
        identifier = ctx.accounts[0].id  # AccountInfo.id ì‚¬ìš©
    elif ctx.profile_name:
        identifier = ctx.profile_name
    else:
        identifier = "default"

    # OutputPath.build()ëŠ” ë¬¸ìì—´(str)ì„ ë°˜í™˜
    output_path = OutputPath(identifier).sub("elb", "log").with_date().build()
    return output_path


def _generate_report_filename(analyzer, analysis_results: dict[str, Any]) -> str:
    """ë³´ê³ ì„œ íŒŒì¼ëª… ìƒì„±"""
    import secrets

    try:
        # ì‹œê°„ ë²”ìœ„ ì •ë³´
        start_dt = analyzer.start_datetime
        end_dt = analyzer.end_datetime
        time_diff = end_dt - start_dt
        hours = int(time_diff.total_seconds() / 3600)

        if hours < 24:
            pass
        else:
            hours // 24
            hours % 24

        # ê³„ì •/ë¦¬ì „ ì •ë³´
        account_id = "unknown"
        region = "unknown"

        s3_uri = f"s3://{analyzer.bucket_name}/{analyzer.prefix}"
        if "/AWSLogs/" in s3_uri:
            path = s3_uri.replace("s3://", "")
            parts = path.split("/AWSLogs/")[1].split("/")
            if len(parts) >= 3:
                account_id = parts[0]
                region = parts[2]

        # ALB ì´ë¦„
        alb_name = analysis_results.get("alb_name") or "alb"
        alb_name = str(alb_name).strip().replace("/", "-").replace("\\", "-")

        # íŒŒì¼ëª… ìƒì„±
        random_suffix = secrets.token_hex(4)
        return f"{account_id}_{region}_{alb_name}_report_{random_suffix}.xlsx"

    except Exception as e:
        logger.debug("Failed to generate report filename: %s", e)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"ALB_Log_Analysis_{timestamp}.xlsx"


def _cleanup_temp_files(analyzer, gz_directory: str, log_directory: str) -> None:
    """ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ë¶„ì„ ì™„ë£Œ í›„ gz, log íŒŒì¼ ì‚­ì œ)"""
    print_sub_info("ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")

    try:
        # 1. analyzer.clean_up í˜¸ì¶œ (DuckDB ë“± ë‚´ë¶€ ë¦¬ì†ŒìŠ¤ ì •ë¦¬)
        if hasattr(analyzer, "clean_up"):
            analyzer.clean_up([])

        # 2. gz ë””ë ‰í† ë¦¬ ë‚´ë¶€ íŒŒì¼ ì‚­ì œ
        if isinstance(gz_directory, str) and os.path.exists(gz_directory):
            try:
                for filename in os.listdir(gz_directory):
                    filepath = os.path.join(gz_directory, filename)
                    if os.path.isfile(filepath):
                        os.remove(filepath)
            except Exception as e:
                logger.debug("Failed to clean gz directory: %s", e)

        # 3. log ë””ë ‰í† ë¦¬ ë‚´ë¶€ íŒŒì¼ ì‚­ì œ
        if isinstance(log_directory, str) and os.path.exists(log_directory):
            try:
                for filename in os.listdir(log_directory):
                    filepath = os.path.join(log_directory, filename)
                    if os.path.isfile(filepath):
                        os.remove(filepath)
            except Exception as e:
                logger.debug("Failed to clean log directory: %s", e)

        print_sub_task_done("ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")

    except Exception as e:
        logger.debug("Failed to cleanup temp files: %s", e)
